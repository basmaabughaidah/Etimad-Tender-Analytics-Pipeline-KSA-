{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 1 — Smart Setup (Sheets + Auth)\n",
        "# =========================================\n",
        "\n",
        "!pip -q install --upgrade \\\n",
        "  gspread==6.1.4 gspread-formatting==1.2.0 google-auth==2.38.0 google-auth-oauthlib==1.2.1 \\\n",
        "  pandas==2.2.2 requests==2.32.4 beautifulsoup4==4.12.3 lxml==5.3.0 reportlab==4.2.5 matplotlib==3.9.2\n",
        "\n",
        "import os, re, json, math, time, datetime as dt\n",
        "import pandas as pd, gspread\n",
        "from gspread.exceptions import WorksheetNotFound\n",
        "from google.oauth2.service_account import Credentials as SA_Creds\n",
        "from gspread_formatting import set_frozen, CellFormat, Color, format_cell_ranges\n",
        "try:\n",
        "    from google.colab import auth as colab_auth\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_auth = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "KSA_TZ = dt.timezone(dt.timedelta(hours=3))\n",
        "SHEET_ID = \"1d2js0tZAIUzmVnKwlBHjr3NEvCLfsl6urEhKScXyPME\"\n",
        "SHEET_TAB = \"Sheet1\"\n",
        "\n",
        "SCHEMA = [\n",
        "    \"اسم المنافسة\",\n",
        "    \"الرقم المرجعي\",\n",
        "    \"الجهة\",\n",
        "    \"قيمة المنافسة\",\n",
        "    \"اخر موعد للاستفسار\",\n",
        "    \"اخر موعد للتقديم\",\n",
        "    \"الرابط\",\n",
        "    \"تاريخ_الإدراج\"\n",
        "]\n",
        "\n",
        "pd.options.display.max_colwidth = 200\n",
        "pd.options.display.width = 200\n",
        "\n",
        "def today_ksa_date():\n",
        "    return dt.datetime.now(KSA_TZ).date().isoformat()\n",
        "\n",
        "def _first_existing(paths):\n",
        "    for p in paths:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _try_service_account():\n",
        "    sa_path = _first_existing([\n",
        "        \"/content/etimad_service_account.json\",\n",
        "        \"/content/drive/MyDrive/etimad_service_account.json\",\n",
        "        \"/workspace/etimad_service_account.json\",\n",
        "        os.getenv(\"GSHEET_SA_JSON\",\"\")\n",
        "    ])\n",
        "    if not sa_path:\n",
        "        return None\n",
        "    creds = SA_Creds.from_service_account_file(\n",
        "        sa_path,\n",
        "        scopes=[\"https://www.googleapis.com/auth/spreadsheets\",\n",
        "                \"https://www.googleapis.com/auth/drive\"]\n",
        "    )\n",
        "    return gspread.authorize(creds)\n",
        "\n",
        "def _try_colab_adc():\n",
        "    if not IN_COLAB:\n",
        "        return None\n",
        "    try:\n",
        "        colab_auth.authenticate_user()\n",
        "        import google.auth\n",
        "        from google.auth.transport.requests import Request\n",
        "        creds, _ = google.auth.default(scopes=[\n",
        "            \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "            \"https://www.googleapis.com/auth/drive\"\n",
        "        ])\n",
        "        if hasattr(creds, \"expired\") and creds.expired and creds.refresh_token:\n",
        "            creds.refresh(Request())\n",
        "        return gspread.authorize(creds)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _try_oauth_file():\n",
        "    cred_path = _first_existing([\n",
        "        \"/content/credentials.json\",\n",
        "        \"/content/drive/MyDrive/credentials.json\",\n",
        "        os.getenv(\"GSHEET_OAUTH_JSON\",\"\")\n",
        "    ])\n",
        "    if not cred_path:\n",
        "        return None\n",
        "    return gspread.oauth(\n",
        "        scopes=[\n",
        "            \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "            \"https://www.googleapis.com/auth/drive\"\n",
        "        ],\n",
        "        credentials_filename=cred_path,\n",
        "        authorized_user_filename=\"/content/authorized_user.json\",\n",
        "    )\n",
        "\n",
        "def get_gspread_client():\n",
        "    for fn in (_try_service_account, _try_colab_adc, _try_oauth_file):\n",
        "        cli = fn()\n",
        "        if cli is not None:\n",
        "            return cli\n",
        "    raise RuntimeError(\"No Google credentials available. Provide a Service Account JSON or credentials.json, or run in Colab and grant access.\")\n",
        "\n",
        "def open_or_create_worksheet(gc, sheet_id: str, tab_name: str):\n",
        "    sh = gc.open_by_key(sheet_id)\n",
        "    try:\n",
        "        ws = sh.worksheet(tab_name)\n",
        "    except WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=tab_name, rows=1000, cols=max(8, len(SCHEMA)))\n",
        "    return ws\n",
        "\n",
        "def ensure_headers(ws):\n",
        "    existing = ws.row_values(1)\n",
        "    if existing != SCHEMA:\n",
        "        if ws.col_count < len(SCHEMA):\n",
        "            ws.resize(rows=ws.row_count, cols=len(SCHEMA))\n",
        "        ws.update('A1', [SCHEMA])\n",
        "    set_frozen(ws, rows=1, cols=0)\n",
        "    header_fmt = CellFormat(backgroundColor=Color(0.85, 0.93, 0.98))\n",
        "    format_cell_ranges(ws, [('A1:{}1'.format(chr(ord('A')+len(SCHEMA)-1)), header_fmt)])\n",
        "\n",
        "def sanity_print(ws):\n",
        "    print(f\"Connected to sheet: https://docs.google.com/spreadsheets/d/{SHEET_ID}\")\n",
        "    print(f\"Worksheet: {SHEET_TAB}\")\n",
        "    print(f\"Columns (A1 headers): {ws.row_values(1)}\")\n",
        "    print(\"Today (KSA):\", today_ksa_date())\n",
        "\n",
        "gc = get_gspread_client()\n",
        "ws = open_or_create_worksheet(gc, SHEET_ID, SHEET_TAB)\n",
        "ensure_headers(ws)\n",
        "sanity_print(ws)\n",
        "print(\"✅ Setup done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYYdtUOJ9BnD",
        "outputId": "8f98f37d-6a93-4082-e1d2-1ea4e04f6b0c"
      },
      "id": "iYYdtUOJ9BnD",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qsjJsw1ov-hb"
      },
      "id": "qsjJsw1ov-hb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 2 — Sheet utilities (today tenders , cleanup, highlight) — FIX v2\n",
        "# =========================================\n",
        "\n",
        "import datetime as _dt\n",
        "from gspread_formatting import CellFormat, Color, format_cell_ranges\n",
        "\n",
        "DATE_FMT = \"%Y-%m-%d\"\n",
        "\n",
        "def _coerce_date_series(s):\n",
        "    s = pd.to_datetime(s, errors=\"coerce\")\n",
        "    s = s.dt.strftime(DATE_FMT).fillna(\"\")\n",
        "    return s\n",
        "\n",
        "def _coerce_value_series(s):\n",
        "    s = s.astype(str).str.replace(r\"[^0-9.]\", \"\", regex=True)\n",
        "    s = s.replace(\"\", pd.NA)\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    return s\n",
        "\n",
        "def _ensure_schema_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in SCHEMA:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.NA\n",
        "    return df[SCHEMA].copy()\n",
        "\n",
        "def _clear_body(ws):\n",
        "    if ws.row_count > 1:\n",
        "        ws.batch_clear([f\"A2:{chr(ord('A')+len(SCHEMA)-1)}{ws.row_count}\"])\n",
        "\n",
        "def _append_rows(ws, rows):\n",
        "    if not rows:\n",
        "        return\n",
        "    ws.append_rows(rows, value_input_option=\"RAW\", table_range=\"A1\")\n",
        "\n",
        "def _read_all_as_df(ws) -> pd.DataFrame:\n",
        "    vals = ws.get_all_values()\n",
        "    if not vals:\n",
        "        return pd.DataFrame(columns=SCHEMA)\n",
        "    header = vals[0]\n",
        "    data = vals[1:]\n",
        "    df = pd.DataFrame(data, columns=header)\n",
        "    for col in SCHEMA:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.NA\n",
        "    return df[SCHEMA].copy()\n",
        "\n",
        "def weekly_cleanup(ws, today_str: str):\n",
        "    df = _read_all_as_df(ws)\n",
        "    if df.empty:\n",
        "        return\n",
        "    dates = pd.to_datetime(df[\"تاريخ_الإدراج\"], errors=\"coerce\")\n",
        "    cutoff = pd.to_datetime(today_str) - pd.Timedelta(days=7)\n",
        "    keep_mask = dates.isna() | (dates >= cutoff)\n",
        "    new_df = df[keep_mask].copy()\n",
        "    _clear_body(ws)\n",
        "    rows = new_df.fillna(\"\").values.tolist()\n",
        "    _append_rows(ws, rows)\n",
        "\n",
        "def highlight_today_rows(ws, today_str: str):\n",
        "    last_row = ws.row_count\n",
        "    if last_row < 2:\n",
        "        return\n",
        "    rng = f\"A2:{chr(ord('A')+len(SCHEMA)-1)}{last_row}\"\n",
        "    fmt = CellFormat(backgroundColor=Color(0.98, 0.98, 0.90))\n",
        "    format_cell_ranges(ws, [(rng, fmt)])\n",
        "\n",
        "def write_today_only(ws, df_today: pd.DataFrame, today_str: str):\n",
        "    df_today = df_today.copy()\n",
        "    if \"قيمة المنافسة\" in df_today.columns:\n",
        "        df_today[\"قيمة المنافسة\"] = _coerce_value_series(df_today[\"قيمة المنافسة\"])\n",
        "    for c in [\"اخر موعد للاستفسار\", \"اخر موعد للتقديم\"]:\n",
        "        if c in df_today.columns:\n",
        "            df_today[c] = _coerce_date_series(df_today[c])\n",
        "    df_today[\"تاريخ_الإدراج\"] = today_str\n",
        "    df_today = _ensure_schema_columns(df_today).fillna(\"\")\n",
        "    _clear_body(ws)\n",
        "    _append_rows(ws, df_today.values.tolist())\n",
        "    highlight_today_rows(ws, today_str)\n",
        "\n",
        "def sync_today_sheet(df_today: pd.DataFrame):\n",
        "    today_str = today_ksa_date()\n",
        "    weekly_cleanup(ws, today_str)\n",
        "    write_today_only(ws, df_today, today_str)\n",
        "    print(f\"Synced today-only rows: {len(df_today)}\")\n",
        "    print(\"Highlight applied.\")\n",
        "    print(\"Weekly cleanup done (older than 7 days).\")\n"
      ],
      "metadata": {
        "id": "KFneYb6m_kXl"
      },
      "id": "KFneYb6m_kXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Cell 3 — Etimad scraper (requests list-cards → fields; keyword filter; fallback)\n",
        "# =========================================\n",
        "\n",
        "import re, time, random\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "LIST_PAGES = [\n",
        "    \"https://tenders.etimad.sa/Tender/AllTendersForVisitor\",\n",
        "    \"https://portal.etimad.sa/Tender/AllTendersForVisitor\",\n",
        "]\n",
        "UA_POOL = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\",\n",
        "]\n",
        "\n",
        "AR_KW = [\n",
        "    \"هاكاثون\",\"هاكثون\",\"الهاكاثون\",\"ثون\",\"ثونات\",\"الهاكاثونات\",\"هاكثونات\",\"هاكاثونات\",\n",
        "    \"ابتكار\",\"يبتكرون\",\"الابتكار\",\"مبتكرون\",\"نبتكر\",\"ابتكارات\",\"المبتكرون\",\"ابتكاري\",\n",
        "    \"فعاليات\",\"فعالية\",\"فعاليه\",\"الفعاليات\",\"الفعالية\",\"الفعاليه\",\n",
        "]\n",
        "EN_KW = [\"hackathon\",\"innovation\",\"innovate\",\"innovator\",\"innovators\",\"event\",\"events\",\"activity\",\"activities\"]\n",
        "\n",
        "_AR_DIAC = re.compile(r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u065F]\")\n",
        "def _norm_ar(s):\n",
        "    s = str(s or \"\")\n",
        "    s = _AR_DIAC.sub(\"\", s)\n",
        "    s = s.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
        "    s = s.replace(\"ى\",\"ي\").replace(\"ئ\",\"ي\").replace(\"ؤ\",\"و\").replace(\"ة\",\"ه\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "def _norm_all(s): return _norm_ar(s).lower()\n",
        "def _kw_ok(name, entity):\n",
        "    t = _norm_all((name or \"\") + \" \" + (entity or \"\"))\n",
        "    for k in AR_KW:\n",
        "        if _norm_ar(k) in t: return True\n",
        "    for k in EN_KW:\n",
        "        if k.lower() in t: return True\n",
        "    return False\n",
        "\n",
        "def _session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\n",
        "        \"User-Agent\": random.choice(UA_POOL),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"ar-SA,ar;q=0.9,en;q=0.8\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "    })\n",
        "    return s\n",
        "def _get(s, url, params=None, sleep=0.7, timeout=45):\n",
        "    r = s.get(url, params=params, timeout=timeout, allow_redirects=True)\n",
        "    r.raise_for_status()\n",
        "    if sleep: time.sleep(sleep)\n",
        "    return r\n",
        "\n",
        "def _parse_date_iso(text):\n",
        "    txt = str(text or \"\")\n",
        "    m = re.search(r\"(\\d{4})[/-](\\d{2})[/-](\\d{2})\", txt)\n",
        "    if m: return f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\"\n",
        "    m = re.search(r\"(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})\", txt)\n",
        "    if m:\n",
        "        d,mn,yy = m.groups(); yy = \"20\"+yy if len(yy)==2 else yy\n",
        "        return f\"{int(yy):04d}-{int(mn):02d}-{int(d):02d}\"\n",
        "    return \"\"\n",
        "\n",
        "def _parse_money(text):\n",
        "    if text is None: return None\n",
        "    t = re.sub(r\"[^\\d.]\", \"\", str(text))\n",
        "    return None if t==\"\" else float(t)\n",
        "\n",
        "def _closest_card(a):\n",
        "    node = a\n",
        "    for _ in range(6):\n",
        "        if not node: break\n",
        "        if node.name in (\"div\",\"section\",\"article\") and (\"card\" in \" \".join(node.get(\"class\", [])) or \"media\" in \" \".join(node.get(\"class\", []))):\n",
        "            return node\n",
        "        node = node.parent\n",
        "    return a.find_parent([\"div\",\"section\",\"article\"]) or a.parent\n",
        "\n",
        "def _extract_from_card(base_url, card, details_href):\n",
        "    name = \"\"\n",
        "    for sel in [\"a.tender-title\",\"a.title\",\"h1\",\"h2\",\"h3\",\".tender-title\",\".title\",\"a\"]:\n",
        "        el = card.select_one(sel)\n",
        "        if el and el.get_text(strip=True) and \"تفاصيل\" not in el.get_text(strip=True):\n",
        "            name = el.get_text(\" \", strip=True)\n",
        "            break\n",
        "    if not name:\n",
        "        name = card.get_text(\" \", strip=True).split(\"\\n\")[0].strip()\n",
        "    text = card.get_text(\" \", strip=True)\n",
        "    ref = \"\"\n",
        "    m = re.search(r\"(?:الرقم\\s*المرجعي|Reference|Tender\\s*No)\\s*[:\\-]?\\s*([A-Za-z0-9\\/\\-]{6,})\", text)\n",
        "    if m: ref = m.group(1).strip()\n",
        "    entity = \"\"\n",
        "    m = re.search(r\"(?:الجهة|Entity|الجهة\\s*الحكومية)\\s*[:\\-]?\\s*([^\\|]+?)(?:\\s{2,}|\\|$)\", text)\n",
        "    if m: entity = m.group(1).strip()\n",
        "    value = None\n",
        "    m = re.search(r\"(?:قيمة\\s*وثائق\\s*المنافسة|Value)\\s*[:\\-]?\\s*([0-9\\.,]+)\", text)\n",
        "    if m: value = _parse_money(m.group(1))\n",
        "    inq = \"\"\n",
        "    m = re.search(r\"(?:آخر\\s*موعد.*?الاستفسارات|اخر\\s*موعد.*?الاستفسارات|Enquir\\w*)\\s*[:\\-]?\\s*([0-9/\\-]{8,10})\", text)\n",
        "    if m: inq = _parse_date_iso(m.group(1))\n",
        "    sub = \"\"\n",
        "    m = re.search(r\"(?:آخر\\s*موعد.*?تقديم|اخر\\s*موعد.*?التقديم|Submission|Closing)\\s*[:\\-]?\\s*([0-9/\\-]{8,10})\", text)\n",
        "    if m: sub = _parse_date_iso(m.group(1))\n",
        "    link = urljoin(base_url, details_href) if details_href else base_url\n",
        "    return {\n",
        "        \"اسم المنافسة\": name or \"\",\n",
        "        \"الرقم المرجعي\": ref or \"\",\n",
        "        \"الجهة\": entity or \"\",\n",
        "        \"قيمة المنافسة\": value,\n",
        "        \"اخر موعد للاستفسار\": inq or \"\",\n",
        "        \"اخر موعد للتقديم\": sub or \"\",\n",
        "        \"الرابط\": link,\n",
        "    }\n",
        "\n",
        "def scrape_list_cards(max_pages=6, per_page_delay=0.7, max_rows=250):\n",
        "    s = _session()\n",
        "    rows = []\n",
        "    seen_links = set()\n",
        "    for base in LIST_PAGES:\n",
        "        for pn in range(1, max_pages+1):\n",
        "            try:\n",
        "                r = _get(s, base, params={\"PageNumber\": pn}, sleep=per_page_delay)\n",
        "            except Exception:\n",
        "                continue\n",
        "            soup = BeautifulSoup(r.text, \"lxml\")\n",
        "            for a in soup.find_all(\"a\", string=lambda x: x and \"تفاصيل\" in x):\n",
        "                href = a.get(\"href\") or \"\"\n",
        "                if \"/Tender/Details\" not in href:\n",
        "                    continue\n",
        "                card = _closest_card(a)\n",
        "                rec = _extract_from_card(base, card, href)\n",
        "                if not rec[\"اسم المنافسة\"] or re.search(r\"(بحث|search|المنافسات)$\", rec[\"اسم المنافسة\"], re.I):\n",
        "                    continue\n",
        "                if rec[\"الرابط\"] in seen_links:\n",
        "                    continue\n",
        "                seen_links.add(rec[\"الرابط\"])\n",
        "                rows.append(rec)\n",
        "                if len(rows) >= max_rows:\n",
        "                    break\n",
        "            if len(rows) >= max_rows:\n",
        "                break\n",
        "        if rows:\n",
        "            break\n",
        "    df = pd.DataFrame(rows, columns=[\n",
        "        \"اسم المنافسة\",\"الرقم المرجعي\",\"الجهة\",\"قيمة المنافسة\",\n",
        "        \"اخر موعد للاستفسار\",\"اخر موعد للتقديم\",\"الرابط\"\n",
        "    ])\n",
        "    return df\n",
        "\n",
        "def build_today_df_from_scrape():\n",
        "    df_all = scrape_list_cards(max_pages=10, max_rows=400)\n",
        "    if df_all.empty:\n",
        "        print(\"List parse produced 0 rows.\")\n",
        "        return df_all.assign(**{\"تاريخ_الإدراج\": today_ksa_date()})\n",
        "    df_kw = df_all[df_all.apply(lambda r: _kw_ok(r[\"اسم المنافسة\"], r[\"الجهة\"]), axis=1)].copy()\n",
        "    out = df_kw if not df_kw.empty else df_all.head(30).copy()\n",
        "    out[\"تاريخ_الإدراج\"] = today_ksa_date()\n",
        "    return out\n",
        "\n",
        "df_today = build_today_df_from_scrape()\n",
        "if len(df_today) > 0:\n",
        "    sync_today_sheet(df_today)\n",
        "    print(f\"Rows written today: {len(df_today)}\")\n",
        "else:\n",
        "    print(\"No rows extracted.\")\n"
      ],
      "metadata": {
        "id": "E_rCbjZGu_mM"
      },
      "id": "E_rCbjZGu_mM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qaQpVIIDdQR8",
      "metadata": {
        "id": "qaQpVIIDdQR8"
      },
      "outputs": [],
      "source": [
        "!pip -q install requests beautifulsoup4 lxml dateparser google-api-python-client google-auth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61cCixbLeJKb",
      "metadata": {
        "id": "61cCixbLeJKb"
      },
      "outputs": [],
      "source": [
        "!pip -q install fuzzywuzzy==0.18.0 python-Levenshtein==0.25.1 arabic_reshaper==3.0.0 python-bidi==0.6.7 reportlab==4.4.4 fpdf2==2.8.4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "QNsrH4VOfTss"
      },
      "id": "QNsrH4VOfTss",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Open existing Google Sheet by ID (OAuth) ===\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "SHEET_ID = \"1d2js0tZAIUzmVnKwlBHjr3NEvCLfsl6urEhKScXyPME\"  # Ghofranai\n",
        "SHEET_TAB = \"Sheet1\"  # change if needed\n",
        "\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "wb = gc.open_by_key(SHEET_ID)\n",
        "ws = wb.worksheet(SHEET_TAB) if SHEET_TAB in [w.title for w in wb.worksheets()] else wb.sheet1\n",
        "\n",
        "print(\"Connected:\", wb.title, \"→\", ws.title)\n",
        "print(\"SHEET_ID:\", SHEET_ID)\n",
        "print(\"Header:\", ws.row_values(1))\n"
      ],
      "metadata": {
        "id": "c4kHCFVLh_Tm"
      },
      "id": "c4kHCFVLh_Tm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import gspread\n",
        "import google.auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "from config import SHEET_ID, SHEET_TAB, SHEET_COLUMNS\n",
        "\n",
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/drive\",\n",
        "]\n",
        "\n",
        "def get_gspread_client():\n",
        "    try:\n",
        "        creds = Credentials.from_service_account_file(\"credentials.json\", scopes=SCOPES)\n",
        "        return gspread.authorize(creds)\n",
        "    except Exception:\n",
        "        creds, _ = google.auth.default(scopes=SCOPES)\n",
        "        if getattr(creds, \"requires_scopes\", False):\n",
        "            creds = creds.with_scopes(SCOPES)\n",
        "        if getattr(creds, \"expired\", False) and getattr(creds, \"refresh_token\", None):\n",
        "            creds.refresh(Request())\n",
        "        return gspread.authorize(creds)\n",
        "\n",
        "gc = get_gspread_client()\n",
        "sh = gc.open_by_key(SHEET_ID)\n",
        "ws = sh.worksheet(SHEET_TAB)\n",
        "\n",
        "try:\n",
        "    headers = ws.row_values(1)\n",
        "except gspread.exceptions.APIError:\n",
        "    headers = []\n",
        "\n",
        "expected_headers = SHEET_COLUMNS\n",
        "if headers != expected_headers:\n",
        "    ws.clear()\n",
        "    ws.update(\"A1:G1\", [expected_headers])"
      ],
      "metadata": {
        "id": "0WVEbwhjl_D1"
      },
      "id": "0WVEbwhjl_D1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb6c7623"
      },
      "source": [
        "# Create a placeholder config.py file\n",
        "# Replace these with your actual configuration values\n",
        "config_content = \"\"\"\n",
        "SHEET_ID = \"YOUR_SHEET_ID\"\n",
        "SHEET_TAB = \"YOUR_SHEET_TAB_NAME\"\n",
        "SHEET_COLUMNS = [\"اسم المنافسة\", \"الرقم المرجعي\", \"الجهة\", \"قيمة المنافسة\", \"اخر موعد للاستفسار\", \"اخر موعد للتقديم\", \"الرابط\", \"تاريخ_الإدراج\"]\n",
        "KEYWORDS = [\"keyword1\", \"keyword2\", \"keyword3\"] # Add your keywords here\n",
        "\"\"\"\n",
        "\n",
        "with open(\"config.py\", \"w\") as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"Created placeholder config.py\")"
      ],
      "id": "fb6c7623",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install rapidfuzz  # optional but recommended\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "try:\n",
        "    from rapidfuzz import fuzz as rf_fuzz\n",
        "    _USE_RAPIDFUZZ = True\n",
        "except Exception:\n",
        "    from fuzzywuzzy import fuzz as fw_fuzz\n",
        "    _USE_RAPIDFUZZ = False\n",
        "\n",
        "_ARABIC_CHAR_MAP = str.maketrans({\n",
        "    \"إ\":\"ا\",\"أ\":\"ا\",\"آ\":\"ا\",\"ى\":\"ي\",\"ئ\":\"ي\",\"ؤ\":\"و\",\"ة\":\"ه\",\"ٱ\":\"ا\",\"ٰ\":\"\"  # dagger alef\n",
        "})\n",
        "\n",
        "def _strip_diacritics(s: str) -> str:\n",
        "    return \"\".join(ch for ch in unicodedata.normalize(\"NFD\", s) if unicodedata.category(ch) != \"Mn\")\n",
        "\n",
        "def _norm_ar(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        s = \"\" if s is None else str(s)\n",
        "    s = s.strip()\n",
        "    s = _strip_diacritics(s)\n",
        "    s = s.replace(\"ـ\", \"\")                   # tatweel\n",
        "    s = s.translate(_ARABIC_CHAR_MAP)        # alef/ya/ta marbuta unification\n",
        "    s = re.sub(r\"\\s+\", \" \", s)               # collapse spaces\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)           # drop punctuation\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s.lower()\n",
        "\n",
        "def _score(a: str, b: str) -> float:\n",
        "    if _USE_RAPIDFUZZ:\n",
        "        return max(\n",
        "            rf_fuzz.ratio(a, b),\n",
        "            rf_fuzz.partial_ratio(a, b),\n",
        "            rf_fuzz.token_sort_ratio(a, b),\n",
        "            rf_fuzz.token_set_ratio(a, b),\n",
        "        )\n",
        "    else:\n",
        "        return max(\n",
        "            fw_fuzz.ratio(a, b),\n",
        "            fw_fuzz.partial_ratio(a, b),\n",
        "            fw_fuzz.token_sort_ratio(a, b),\n",
        "            fw_fuzz.token_set_ratio(a, b),\n",
        "        )\n",
        "\n",
        "def fuzzy_match_keywords(text, keywords, threshold=70):\n",
        "    \"\"\"\n",
        "    Returns (matched: bool, best_keyword: str|None, score: float).\n",
        "    Arabic-aware normalization + fast substring check before fuzzy.\n",
        "    \"\"\"\n",
        "    if not keywords:\n",
        "        return False, None, 0.0\n",
        "\n",
        "    t = _norm_ar(text)\n",
        "    if not t:\n",
        "        return False, None, 0.0\n",
        "\n",
        "    # Pre-normalize keywords once\n",
        "    norm_kw = [(kw, _norm_ar(kw)) for kw in keywords if isinstance(kw, (str, bytes))]\n",
        "\n",
        "    # 1) Fast containment check\n",
        "    for raw_kw, kw in norm_kw:\n",
        "        if kw and (kw in t or t in kw):\n",
        "            return True, raw_kw, 100.0\n",
        "\n",
        "    # 2) Fuzzy best-match\n",
        "    best_kw, best_sc = None, 0.0\n",
        "    for raw_kw, kw in norm_kw:\n",
        "        if not kw:\n",
        "            continue\n",
        "        sc = _score(t, kw)\n",
        "        if sc > best_sc:\n",
        "            best_sc, best_kw = sc, raw_kw\n",
        "\n",
        "    if best_sc >= float(threshold):\n",
        "        return True, best_kw, float(best_sc)\n",
        "    return False, None, float(best_sc)\n"
      ],
      "metadata": {
        "id": "j-wbY3K3s7E4"
      },
      "id": "j-wbY3K3s7E4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585511d8",
      "metadata": {
        "id": "585511d8"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any, Tuple\n",
        "import time\n",
        "\n",
        "def get_worksheet_data_and_indices_safely(worksheet, expected_headers: List[str] = None) -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Read all rows from a gspread worksheet with retries.\n",
        "    Returns:\n",
        "      - rows: list of dicts (including 'original_row_number')\n",
        "      - headers: the header row used for mapping\n",
        "    \"\"\"\n",
        "    max_retries = 3\n",
        "    delay = 1.0\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            print(f\"Fetching data (attempt {attempt}/{max_retries}) ...\")\n",
        "            all_values = worksheet.get_all_values()\n",
        "\n",
        "            if not all_values:\n",
        "                print(\"Worksheet is empty.\")\n",
        "                return [], []\n",
        "\n",
        "            headers = all_values[0] if all_values[0] else []\n",
        "            if expected_headers and headers != expected_headers:\n",
        "                print(\"Header mismatch detected. Using sheet headers as-is.\")\n",
        "\n",
        "            if len(all_values) < 2:\n",
        "                print(\"No data rows found.\")\n",
        "                return [], headers\n",
        "\n",
        "            rows: List[Dict[str, Any]] = []\n",
        "            for row_idx, row in enumerate(all_values[1:], start=2):\n",
        "                row_dict = {}\n",
        "                for j, h in enumerate(headers):\n",
        "                    row_dict[h] = row[j] if j < len(row) else \"\"\n",
        "                row_dict[\"original_row_number\"] = row_idx\n",
        "                rows.append(row_dict)\n",
        "\n",
        "            print(f\"Fetched {len(rows)} rows.\")\n",
        "            return rows, headers\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            print(f\"Error reading worksheet: {e}\")\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "\n",
        "    raise last_err\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6f1596",
      "metadata": {
        "id": "6b6f1596"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from fuzzywuzzy import fuzz, process\n",
        "import os\n",
        "import time\n",
        "\n",
        "def fuzzy_match_keywords(text: str, keywords: List[str], threshold: int = 90) -> Tuple[bool, str, int]:\n",
        "    \"\"\"\n",
        "    Return (is_match, best_keyword, score) using token_set_ratio.\n",
        "    \"\"\"\n",
        "    text = (text or \"\").strip()\n",
        "    if not text or not keywords:\n",
        "        return False, \"\", 0\n",
        "    best_kw, score = process.extractOne(text, keywords, scorer=fuzz.token_set_ratio)\n",
        "    return (score >= threshold), best_kw, score\n",
        "\n",
        "def filter_and_delete_rows(\n",
        "    worksheet,\n",
        "    keywords_file: str,\n",
        "    target_columns: List[str] = None,\n",
        "    similarity_threshold: int = 90,\n",
        "    dry_run: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Filter rows by checking if any of target_columns fuzzy-matches any keyword.\n",
        "    Rows without a match are deleted (unless dry_run=True).\n",
        "    \"\"\"\n",
        "    if target_columns is None:\n",
        "        target_columns = [\"النشاط الاساسي\"]\n",
        "\n",
        "    # Load keywords\n",
        "    if not os.path.exists(keywords_file):\n",
        "        raise FileNotFoundError(f\"Keywords file not found: {keywords_file}\")\n",
        "    keywords = read_keywords_from_file(keywords_file)\n",
        "    if not keywords:\n",
        "        print(\"No keywords loaded.\")\n",
        "        return\n",
        "\n",
        "    # Read sheet rows\n",
        "    rows, headers = get_worksheet_data_and_indices_safely(worksheet, expected_headers=SHEET_COLUMNS)\n",
        "    if not rows:\n",
        "        print(\"No data rows to process.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing {len(rows)} rows\")\n",
        "    print(f\"Target columns: {target_columns}\")\n",
        "    print(f\"Similarity threshold: {similarity_threshold}%\")\n",
        "\n",
        "    # Validate target columns exist\n",
        "    missing_cols = [c for c in target_columns if c not in (headers or [])]\n",
        "    if missing_cols:\n",
        "        print(f\"Warning: target columns missing in sheet: {missing_cols}\")\n",
        "\n",
        "    rows_to_delete = []\n",
        "    for row in rows:\n",
        "        keep = False\n",
        "        for col in target_columns:\n",
        "            if col in row:\n",
        "                val = row.get(col, \"\")\n",
        "                is_match, best_kw, score = fuzzy_match_keywords(val, keywords, similarity_threshold)\n",
        "                if is_match:\n",
        "                    keep = True\n",
        "                    break\n",
        "        if not keep:\n",
        "            rows_to_delete.append(row[\"original_row_number\"])\n",
        "\n",
        "    print(\"\\n=== SUMMARY ===\")\n",
        "    print(f\"Total rows: {len(rows)}\")\n",
        "    print(f\"Rows to delete: {len(rows_to_delete)}\")\n",
        "    print(f\"Rows to keep: {len(rows) - len(rows_to_delete)}\")\n",
        "\n",
        "    if dry_run:\n",
        "        print(\"\\n=== DRY RUN ===\")\n",
        "        if rows_to_delete:\n",
        "            print(f\"Rows that would be deleted (desc): {sorted(rows_to_delete, reverse=True)}\")\n",
        "        else:\n",
        "            print(\"No rows would be deleted.\")\n",
        "        return\n",
        "\n",
        "    if not rows_to_delete:\n",
        "        print(\"No rows to delete.\")\n",
        "        return\n",
        "\n",
        "    # Perform deletions (descending order to keep indices stable)\n",
        "    deleted = 0\n",
        "    for r in sorted(rows_to_delete, reverse=True):\n",
        "        try:\n",
        "            worksheet.delete_rows(r)\n",
        "            deleted += 1\n",
        "            time.sleep(0.8)  # rate limit safety\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting row {r}: {e}\")\n",
        "\n",
        "    print(f\"\\nDeleted {deleted} of {len(rows_to_delete)} rows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4bb001b",
      "metadata": {
        "id": "c4bb001b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from config import SHEET_ID as CFG_SHEET_ID, SHEET_TAB as CFG_SHEET_TAB, KEYWORDS as CFG_KEYWORDS\n",
        "\n",
        "credentials_file = \"credentials.json\"\n",
        "sheet_id = \"15eOK-kuB2zGOWsNCo1WTu28Xgb8L9Kqzib2UMrHtiwU\" or CFG_SHEET_ID\n",
        "keywords_file = \"keywords.txt\"\n",
        "\n",
        "# Ensure keywords file exists (one keyword per line)\n",
        "if not os.path.exists(keywords_file):\n",
        "    with open(keywords_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for kw in CFG_KEYWORDS:\n",
        "            f.write(f\"{kw}\\n\")\n",
        "\n",
        "print(f\"Credentials file: {os.path.abspath(credentials_file)}\")\n",
        "print(f\"Sheet ID: {sheet_id}\")\n",
        "print(f\"Keywords file: {os.path.abspath(keywords_file)}\")\n",
        "\n",
        "# Quick connectivity check and worksheet handle\n",
        "client, workbook = setup_connection(credentials_file, sheet_id)\n",
        "worksheet = open_worksheet(workbook, CFG_SHEET_TAB)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword-based row filtering for the new schema (Ghofranai)\n",
        "\n",
        "TARGET_COLUMNS = ['اسم المنافسة']      # new column to match against\n",
        "SIMILARITY_THRESHOLD = 95               # keep as needed (90–95 typical)\n",
        "DRY_RUN = False                         # set True to preview without deleting\n",
        "\n",
        "# Reuse existing ws if present; otherwise open by SHEET_ID/SHEET_TAB\n",
        "try:\n",
        "    _ = ws.title  # ensure ws exists\n",
        "except NameError:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    import gspread\n",
        "    from google.auth import default\n",
        "\n",
        "    SHEET_ID = \"1d2js0tZAIUzmVnKwlBHjr3NEvCLfsl6urEhKScXyPME\"  # Ghofranai\n",
        "    SHEET_TAB = \"Sheet1\"\n",
        "\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    wb = gc.open_by_key(SHEET_ID)\n",
        "    ws = wb.worksheet(SHEET_TAB) if SHEET_TAB in [w.title for w in wb.worksheets()] else wb.sheet1\n",
        "\n",
        "# Run filtering\n",
        "try:\n",
        "    # keywords_file must point to a valid text file with one keyword per line\n",
        "    filter_and_delete_rows(\n",
        "        worksheet=ws,\n",
        "        keywords_file=keywords_file,\n",
        "        target_columns=TARGET_COLUMNS,\n",
        "        similarity_threshold=SIMILARITY_THRESHOLD,\n",
        "        dry_run=DRY_RUN\n",
        "    )\n",
        "    print(f\"Completed filtering on {TARGET_COLUMNS} | dry_run={DRY_RUN}\")\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(f\"Filtering failed: {e}\")\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "FtdevGi8kot2"
      },
      "id": "FtdevGi8kot2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8510d1d",
      "metadata": {
        "id": "e8510d1d"
      },
      "outputs": [],
      "source": [
        "!pip -q install PyPDF2==3.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741ab86d",
      "metadata": {
        "id": "741ab86d"
      },
      "outputs": [],
      "source": [
        "# Robust PDF reader: handles empty pages, whitespace, and encoding quirks\n",
        "from typing import Optional\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "def read_pdf_content(pdf_path: str) -> Optional[str]:\n",
        "    \"\"\"Return concatenated text from all pages in a PDF, or None if failed.\"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"Error: File not found: {pdf_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as fh:\n",
        "            reader = PyPDF2.PdfReader(fh)\n",
        "            parts = []\n",
        "            for i, page in enumerate(reader.pages):\n",
        "                try:\n",
        "                    txt = page.extract_text() or \"\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: failed to extract page {i}: {e}\")\n",
        "                    txt = \"\"\n",
        "                parts.append(txt.strip())\n",
        "            content = \"\\n\".join([p for p in parts if p])\n",
        "            content = \"\\n\".join(line.strip() for line in content.splitlines())\n",
        "            return content if content.strip() else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "pdf_file_path = \"/content/keywords.pdf\"\n",
        "pdf_text_data = read_pdf_content(pdf_file_path)\n",
        "\n",
        "if pdf_text_data is not None:\n",
        "    print(\"Successfully read PDF content into 'pdf_text_data'.\")\n",
        "    # Preview first 500 characters if needed:\n",
        "    # print(pdf_text_data[:500])\n",
        "else:\n",
        "    print(\"Failed to read PDF content.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data directly from Google Sheet (OAuth)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "SHEET_ID = \"1d2js0tZAIUzmVnKwlBHjr3NEvCLfsl6urEhKScXyPME\"  # Ghofranai\n",
        "SHEET_TAB = \"Sheet1\"  # change if needed\n",
        "\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "wb = gc.open_by_key(SHEET_ID)\n",
        "ws = wb.worksheet(SHEET_TAB) if SHEET_TAB in [w.title for w in wb.worksheets()] else wb.sheet1\n",
        "\n",
        "vals = ws.get_all_values()\n",
        "if not vals or len(vals) < 1:\n",
        "    raise ValueError(\"Sheet appears empty.\")\n",
        "\n",
        "headers = vals[0]\n",
        "rows = vals[1:]\n",
        "data = pd.DataFrame(rows, columns=headers)\n",
        "\n",
        "# Optional: normalize common header variants into the new schema\n",
        "rename_map = {\n",
        "    \"قيمة الكراسة\": \"قيمة المنافسة\",\n",
        "    \"قيمه وثائق المنافسه\": \"قيمة المنافسة\",\n",
        "    \"قيمة وثائق المنافسة\": \"قيمة المنافسة\",\n",
        "    \"اخر توقيت لاستلام الاستفسارات\": \"اخر موعد للاستفسار\",\n",
        "    \"آخر موعد لاستلام الاستفسارات\": \"اخر موعد للاستفسار\",\n",
        "    \"اخر موعد لتقديم العرض\": \"اخر موعد للتقديم\",\n",
        "    \"آخر موعد لتقديم العرض\": \"اخر موعد للتقديم\",\n",
        "    \"URL\": \"الرابط\",\n",
        "    \"url\": \"الرابط\",\n",
        "}\n",
        "present = {k: v for k, v in rename_map.items() if k in data.columns}\n",
        "if present:\n",
        "    data = data.rename(columns=present)\n",
        "\n",
        "# Coerce numeric for \"قيمة المنافسة\"\n",
        "if \"قيمة المنافسة\" in data.columns:\n",
        "    s = (\n",
        "        data[\"قيمة المنافسة\"].astype(str)\n",
        "        .str.replace(r'مجانا|مجان', '0', flags=re.IGNORECASE, regex=True)\n",
        "        .str.replace(r'[^\\d\\.\\-]', '', regex=True)\n",
        "        .str.strip()\n",
        "    )\n",
        "    data[\"قيمة المنافسة\"] = pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "# Parse dates if present\n",
        "for c in [\"اخر موعد للاستفسار\", \"اخر موعد للتقديم\", \"تاريخ نشرها\", \"تاريخ النشر\"]:\n",
        "    if c in data.columns:\n",
        "        data[c] = pd.to_datetime(data[c], errors=\"coerce\")\n",
        "\n",
        "print(len(data), \"rows\")\n",
        "display(data.head(10))\n"
      ],
      "metadata": {
        "id": "-qjHu55Ujg3V"
      },
      "id": "-qjHu55Ujg3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "twlJswrJiECW",
      "metadata": {
        "id": "twlJswrJiECW"
      },
      "outputs": [],
      "source": [
        "# --- 0. EMAIL MODULES ---\n",
        "import smtplib\n",
        "import ssl\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "\n",
        "# --- 0. REPORTLAB & ANALYSIS MODULES (Standard) ---\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak, Image as RlImage\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch\n",
        "from reportlab.lib import colors\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "# Data Analysis Modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import re\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import io\n",
        "import unicodedata\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. GLOBAL CONFIGURATIONS (Shared) ---\n",
        "FONT_FILENAME = 'Amiri-Regular.ttf'\n",
        "ARABIC_FONT_PATH = FONT_FILENAME\n",
        "PLOT_DPI = 150\n",
        "\n",
        "# --- EMAIL CONFIGURATION ---\n",
        "SMTP_SERVER = \"smtp.gmail.com\"\n",
        "SMTP_PORT = 465\n",
        "SENDER_EMAIL = \"ghofranzalqarni@gmail.com\"\n",
        "SENDER_PASSWORD = \"euzysjzccifszhsr\"  # Gmail App Password recommended\n",
        "\n",
        "# Register Arabic font for ReportLab\n",
        "try:\n",
        "    pdfmetrics.registerFont(TTFont('ArabicFont', FONT_FILENAME))\n",
        "except Exception as e:\n",
        "    print(f\"Error registering ReportLab font: {e}. Check if '{FONT_FILENAME}' exists in the working directory.\")\n",
        "\n",
        "# --- 2. ARABIC TEXT UTILITIES (Shared) ---\n",
        "def _prepare_arabic_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    reshaped_text = arabic_reshaper.reshape(text)\n",
        "    bidi_text = get_display(reshaped_text)\n",
        "    return bidi_text\n",
        "\n",
        "# Minimal Arabic normalization for grouping/keys\n",
        "_ARABIC_MAP = str.maketrans({\n",
        "    \"أ\": \"ا\", \"إ\": \"ا\", \"آ\": \"ا\",\n",
        "    \"ى\": \"ي\", \"ي\": \"ي\",\n",
        "    \"ة\": \"ه\",\n",
        "    \"ؤ\": \"و\", \"ئ\": \"ي\",\n",
        "})\n",
        "_ARABIC_DIACRITICS = {chr(c) for c in range(0x0610, 0x061B+1)} | {chr(c) for c in range(0x064B, 0x065F+1)}\n",
        "\n",
        "def _strip_diacritics(s: str) -> str:\n",
        "    return \"\".join(ch for ch in s if ch not in _ARABIC_DIACRITICS)\n",
        "\n",
        "def _normalize_arabic(s):\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = _strip_diacritics(s)\n",
        "    s = s.translate(_ARABIC_MAP)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# --- 3. REPORTLAB STYLES ---\n",
        "styles = getSampleStyleSheet()\n",
        "arabic_style_title = ParagraphStyle(\n",
        "    name='ArabicTitle', parent=styles['Title'], fontName='ArabicFont', fontSize=18, alignment=2, spaceAfter=24, textColor=colors.darkblue\n",
        ")\n",
        "arabic_style_heading2 = ParagraphStyle(\n",
        "    name='ArabicHeading2', parent=styles['Heading2'], fontName='ArabicFont', fontSize=14, alignment=2, spaceBefore=18, spaceAfter=12, textColor=colors.black\n",
        ")\n",
        "arabic_style_normal = ParagraphStyle(\n",
        "    name='ArabicNormal', parent=styles['Normal'], fontName='ArabicFont', fontSize=12, alignment=2, spaceAfter=6, textColor=colors.black\n",
        ")\n",
        "\n",
        "# --- 4. DATA ANALYSIS CLASS ---\n",
        "ARABIC_FONT_PROP = None\n",
        "\n",
        "def _get_arabic_font_prop():\n",
        "    global ARABIC_FONT_PROP\n",
        "    if ARABIC_FONT_PROP is None:\n",
        "        try:\n",
        "            ARABIC_FONT_PROP = FontProperties(fname=ARABIC_FONT_PATH)\n",
        "        except FileNotFoundError:\n",
        "            ARABIC_FONT_PROP = FontProperties(family='sans-serif')\n",
        "    return ARABIC_FONT_PROP\n",
        "\n",
        "class TenderDataAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.df = None\n",
        "        self.font_prop = _get_arabic_font_prop()\n",
        "        self.plot_paths = {}\n",
        "\n",
        "    def load_data_from_text(self, data_text):\n",
        "        self.df = pd.DataFrame(data_text)\n",
        "\n",
        "        date_columns = ['اخر معاد لاستلام الاستفسارات', 'اخر موعد لتقديم العرض', 'تاريخ النشر']\n",
        "        for col in date_columns:\n",
        "            if col in self.df.columns:\n",
        "                self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
        "\n",
        "        value_col = 'قيمه وثائق المنافسه'\n",
        "        if value_col in self.df.columns:\n",
        "            self.df[value_col] = (\n",
        "                self.df[value_col]\n",
        "                .astype(str)\n",
        "                .str.replace(r'مجانا|مجان', '0', flags=re.IGNORECASE, regex=True)\n",
        "                .str.replace(r'[^\\d\\.\\-]', '', regex=True)\n",
        "                .str.strip()\n",
        "            )\n",
        "            self.df[value_col] = pd.to_numeric(self.df[value_col], errors='coerce')\n",
        "\n",
        "        for col in ['النشاط الاساسي', 'النشاط الأساسي', 'العنوان']:\n",
        "            if col in self.df.columns:\n",
        "                new_col = f'{col}_normalized'\n",
        "                self.df[new_col] = self.df[col].apply(_normalize_arabic)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def group_by_activity(self):\n",
        "        activity_col = None\n",
        "        possible_names = ['النشاط الاساسي_normalized', 'النشاط الأساسي_normalized', 'النشاط الاساسي', 'النشاط الأساسي']\n",
        "        for col_name in possible_names:\n",
        "            if col_name in self.df.columns:\n",
        "                activity_col = col_name\n",
        "                break\n",
        "        if activity_col is None:\n",
        "            return None, None\n",
        "\n",
        "        activity_counts = self.df[activity_col].value_counts()\n",
        "        activity_stats = None\n",
        "        value_col = 'قيمه وثائق المنافسه'\n",
        "\n",
        "        if value_col in self.df.columns:\n",
        "            activity_stats = self.df.groupby(activity_col).agg(\n",
        "                Count=(value_col, 'count'),\n",
        "                MeanValue=(value_col, 'mean'),\n",
        "                TotalValue=(value_col, 'sum'),\n",
        "                MinValue=(value_col, 'min'),\n",
        "                MaxValue=(value_col, 'max')\n",
        "            ).round(2)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        labels = [_prepare_arabic_text(label) for label in activity_counts.index]\n",
        "        title = _prepare_arabic_text('عدد المنافسات حسب النشاط الأساسي')\n",
        "        ylabel = _prepare_arabic_text('عدد المنافسات')\n",
        "        plt.bar(range(len(activity_counts)), activity_counts.values)\n",
        "        plt.title(title, fontproperties=self.font_prop)\n",
        "        plt.xticks(range(len(activity_counts)), labels, rotation=45, ha='right', fontproperties=self.font_prop)\n",
        "        plt.ylabel(ylabel, fontproperties=self.font_prop)\n",
        "\n",
        "        if value_col in self.df.columns:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            activity_values = self.df.groupby(activity_col)[value_col].sum()\n",
        "            pie_labels = [_prepare_arabic_text(label) for label in activity_values.index]\n",
        "            pie_title = _prepare_arabic_text('توزيع قيمة الكراسه')\n",
        "            plt.pie(activity_values, labels=pie_labels, autopct='%1.1f%%',\n",
        "                    textprops={'fontproperties': self.font_prop, 'fontsize': 10})\n",
        "            plt.title(pie_title, fontproperties=self.font_prop)\n",
        "            plt.ylabel('')\n",
        "            plt.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=PLOT_DPI)\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        self.plot_paths['activity_plot'] = buf\n",
        "\n",
        "        return activity_counts, activity_stats\n",
        "\n",
        "    def group_by_organization(self):\n",
        "        org_col = 'العنوان_normalized' if 'العنوان_normalized' in self.df.columns else 'العنوان'\n",
        "        if org_col not in self.df.columns:\n",
        "            return None\n",
        "\n",
        "        org_counts = self.df[org_col].value_counts()\n",
        "        top_orgs = org_counts.head(10)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        labels = [_prepare_arabic_text(org) for org in top_orgs.index]\n",
        "        labels = [org[:50] + '...' if isinstance(org, str) and len(org) > 50 else org for org in labels]\n",
        "        xlabel = _prepare_arabic_text('عدد المنافسات')\n",
        "        title = _prepare_arabic_text('أكثر 10 جهات إصداراً للمناقصات')\n",
        "\n",
        "        plt.barh(range(len(top_orgs)), top_orgs.values)\n",
        "        plt.yticks(range(len(top_orgs)), labels, fontproperties=self.font_prop)\n",
        "        plt.xlabel(xlabel, fontproperties=self.font_prop)\n",
        "        plt.title(title, fontproperties=self.font_prop)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=PLOT_DPI)\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        self.plot_paths['organization_plot'] = buf\n",
        "\n",
        "        return org_counts\n",
        "\n",
        "    def analyze_by_date(self):\n",
        "        date_col = 'تاريخ النشر'\n",
        "        if date_col not in self.df.columns:\n",
        "            return None\n",
        "\n",
        "        self.df['شهر النشر'] = self.df[date_col].dt.to_period('M')\n",
        "        monthly_counts = self.df['شهر النشر'].value_counts().sort_index()\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        title1 = _prepare_arabic_text('اتجاه نشر المنافسات شهرياً')\n",
        "        xlabel1 = _prepare_arabic_text('الشهر')\n",
        "        ylabel1 = _prepare_arabic_text('عدد المنافسات')\n",
        "        monthly_counts.plot(kind='line', marker='o')\n",
        "        plt.title(title1, fontproperties=self.font_prop)\n",
        "        plt.xlabel(xlabel1, fontproperties=self.font_prop)\n",
        "        plt.ylabel(ylabel1, fontproperties=self.font_prop)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        self.df['يوم الأسبوع'] = self.df[date_col].dt.dayofweek\n",
        "        day_names_ar = ['الاثنين', 'الثلاثاء', 'الأربعاء', 'الخميس', 'الجمعة', 'السبت', 'الأحد']\n",
        "        day_counts = self.df['يوم الأسبوع'].value_counts().sort_index()\n",
        "\n",
        "        title2 = _prepare_arabic_text('توزيع النشر حسب أيام الأسبوع')\n",
        "        ylabel2 = _prepare_arabic_text('عدد المنافسات')\n",
        "        xlabels = [_prepare_arabic_text(d) for d in [day_names_ar[i] for i in day_counts.index]]\n",
        "        plt.bar(xlabels, day_counts.values)\n",
        "        plt.title(title2, fontproperties=self.font_prop)\n",
        "        plt.xticks(rotation=45, fontproperties=self.font_prop)\n",
        "        plt.ylabel(ylabel2, fontproperties=self.font_prop)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=PLOT_DPI)\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        self.plot_paths['date_analysis_plot'] = buf\n",
        "\n",
        "        return monthly_counts\n",
        "\n",
        "    def analyze_weekly_tender_count_trend(self):\n",
        "        date_col = 'تاريخ النشر'\n",
        "        if date_col not in self.df.columns:\n",
        "            return None\n",
        "\n",
        "        self.df['أسبوع النشر'] = self.df[date_col].dt.to_period('W')\n",
        "        weekly_count = self.df['أسبوع النشر'].value_counts().sort_index()\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        title = _prepare_arabic_text('الاتجاه الأسبوعي لعدد المنافسات')\n",
        "        xlabel = _prepare_arabic_text('أسبوع النشر')\n",
        "        ylabel = _prepare_arabic_text('عدد المنافسات')\n",
        "        weekly_count.plot(kind='line', marker='o', linestyle='-')\n",
        "        plt.title(title, fontproperties=self.font_prop)\n",
        "        plt.xlabel(xlabel, fontproperties=self.font_prop)\n",
        "        plt.ylabel(ylabel, fontproperties=self.font_prop)\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=PLOT_DPI)\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        self.plot_paths['weekly_count_trend_plot'] = buf\n",
        "\n",
        "        return weekly_count\n",
        "\n",
        "    def analyze_weekly_document_value_trend(self):\n",
        "        date_col = 'تاريخ النشر'\n",
        "        value_col = 'قيمه وثائق المنافسه'\n",
        "        if date_col not in self.df.columns or value_col not in self.df.columns:\n",
        "            return None\n",
        "\n",
        "        self.df['أسبوع النشر'] = self.df[date_col].dt.to_period('W')\n",
        "        weekly_value_sum = self.df.groupby('أسبوع النشر')[value_col].sum().sort_index()\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        title = _prepare_arabic_text('الاتجاه الأسبوعي لإجمالي قيمة الكراسه')\n",
        "        xlabel = _prepare_arabic_text('أسبوع النشر')\n",
        "        ylabel = _prepare_arabic_text('إجمالي قيمة الكراسه')\n",
        "        weekly_value_sum.plot(kind='line', marker='o', linestyle='-')\n",
        "        plt.title(title, fontproperties=self.font_prop)\n",
        "        plt.xlabel(xlabel, fontproperties=self.font_prop)\n",
        "        plt.ylabel(ylabel, fontproperties=self.font_prop)\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=PLOT_DPI)\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        self.plot_paths['weekly_value_trend_plot'] = buf\n",
        "\n",
        "        return weekly_value_sum\n",
        "\n",
        "    def analyze_document_values(self):\n",
        "        value_col = 'قيمه وثائق المنافسه'\n",
        "        if value_col not in self.df.columns:\n",
        "            return None, None\n",
        "\n",
        "        values = self.df[value_col].dropna()\n",
        "        summary_stats = {\n",
        "            'Total Value': values.sum(),\n",
        "            'Mean Value': values.mean(),\n",
        "            'Max Value': values.max(),\n",
        "            'Min Value': values.min()\n",
        "        }\n",
        "\n",
        "        bins = [0, 500, 1000, 2000, 5000, float('inf')]\n",
        "        labels = ['0-500', '501-1000', '1001-2000', '2001-5000', '5000+']\n",
        "        self.df['فئة القيمة'] = pd.cut(self.df[value_col], bins=bins, labels=labels, include_lowest=True)\n",
        "        value_distribution = self.df['فئة القيمة'].value_counts().sort_index()\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        title1 = _prepare_arabic_text('توزيع قيم الكراسه المنافسات')\n",
        "        xlabel1 = _prepare_arabic_text('قيمة الكراسه')\n",
        "        ylabel1 = _prepare_arabic_text('التكرار')\n",
        "        plt.hist(values, bins=20, edgecolor='black', alpha=0.7)\n",
        "        plt.title(title1, fontproperties=self.font_prop)\n",
        "        plt.xlabel(xlabel1, fontproperties=self.font_prop)\n",
        "        plt.ylabel(ylabel1, fontproperties=self.font_prop)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        pie_labels = [_prepare_arabic_text(str(label)) for label in value_distribution.index]\n",
        "        pie_title = _prepare_arabic_text('توزيع المنافسات حسب فئات القيمة')\n",
        "        plt.pie(value_distribution, labels=pie_labels, autopct='%1.1f%%',\n",
        "                textprops={'fontproperties': self.font_prop, 'fontsize': 10})\n",
        "        plt.title(pie_title, fontproperties=self.font_prop)\n",
        "        plt.ylabel('')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=PLOT_DPI)\n",
        "        plt.close()\n",
        "        buf.seek(0)\n",
        "        self.plot_paths['value_analysis_plot'] = buf\n",
        "\n",
        "        return summary_stats, value_distribution\n",
        "\n",
        "    def generate_summary_report(self):\n",
        "        summary = {}\n",
        "        summary['Total Tenders'] = len(self.df)\n",
        "\n",
        "        activity_col = None\n",
        "        possible_activity_names = ['النشاط الاساسي_normalized', 'النشاط الأساسي_normalized', 'النشاط الاساسي', 'النشاط الأساسي']\n",
        "        for col_name in possible_activity_names:\n",
        "            if col_name in self.df.columns:\n",
        "                activity_col = col_name\n",
        "                break\n",
        "\n",
        "        org_col = 'العنوان_normalized' if 'العنوان_normalized' in self.df.columns else 'العنوان'\n",
        "\n",
        "        if activity_col:\n",
        "            summary['Unique Activities'] = self.df[activity_col].nunique()\n",
        "            mode_value = self.df[activity_col].mode()\n",
        "            summary['Most Common Activity'] = _prepare_arabic_text(str(mode_value.iloc[0])) if not mode_value.empty else 'N/A'\n",
        "\n",
        "        if org_col in self.df.columns:\n",
        "            summary['Unique Organizations'] = self.df[org_col].nunique()\n",
        "\n",
        "        if 'قيمه وثائق المنافسه' in self.df.columns:\n",
        "            valid_values = self.df['قيمه وثائق المنافسه'].dropna()\n",
        "            summary['Total Value'] = valid_values.sum()\n",
        "            summary['Average Value'] = valid_values.mean()\n",
        "            summary['Min Value'] = valid_values.min()\n",
        "            summary['Max Value'] = valid_values.max()\n",
        "\n",
        "        if 'تاريخ النشر' in self.df.columns and not self.df['تاريخ النشر'].isnull().all():\n",
        "            date_range = self.df['تاريخ النشر'].max() - self.df['تاريخ النشر'].min()\n",
        "            summary['Time Period Days'] = date_range.days\n",
        "\n",
        "        return summary\n",
        "\n",
        "# --- 5. REPORT GENERATION FUNCTION ---\n",
        "def df_to_table_data(df, include_index=False):\n",
        "    header = [_prepare_arabic_text(str(col)) for col in df.columns]\n",
        "    if include_index:\n",
        "        index_name = str(df.index.name) if df.index.name else ''\n",
        "        header.insert(0, _prepare_arabic_text(index_name))\n",
        "\n",
        "    data = [header]\n",
        "    for index, row in df.iterrows():\n",
        "        row_list = []\n",
        "        if include_index:\n",
        "            row_list.append(_prepare_arabic_text(str(index)))\n",
        "        for item in row.values:\n",
        "            if isinstance(item, (int, float, np.number)):\n",
        "                if isinstance(item, int):\n",
        "                    row_list.append(f\"{item:,}\")\n",
        "                else:\n",
        "                    if item == int(item):\n",
        "                        row_list.append(f\"{int(item):,}\")\n",
        "                    else:\n",
        "                        row_list.append(f\"{item:,.2f}\")\n",
        "            else:\n",
        "                row_list.append(_prepare_arabic_text(str(item)))\n",
        "        data.append(row_list)\n",
        "    return data\n",
        "\n",
        "def create_full_pdf_report(analyzer: TenderDataAnalyzer, filename='arabic_data_report.pdf'):\n",
        "    doc = SimpleDocTemplate(\n",
        "        filename, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18\n",
        "    )\n",
        "    story = []\n",
        "    table_style = TableStyle([\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#ADD8E6')),\n",
        "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.black),\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'RIGHT'),\n",
        "        ('FONTNAME', (0, 0), (-1, -1), 'ArabicFont'),\n",
        "        ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
        "        ('BOTTOMPADDING', (0, 0), (-1, 0), 6),\n",
        "        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
        "        ('BACKGROUND', (0, 1), (-1, -1), colors.white),\n",
        "    ])\n",
        "\n",
        "    story.append(Paragraph(_prepare_arabic_text('تقرير تحليلي شامل للمناقصات'), arabic_style_title))\n",
        "    story.append(Spacer(1, 0.5 * inch))\n",
        "    story.append(Paragraph(_prepare_arabic_text(f'تاريخ التقرير: {datetime.now().strftime(\"%Y-%m-%d\")}'), arabic_style_normal))\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    story.append(Paragraph(_prepare_arabic_text('1. الملخص التنفيذي'), arabic_style_heading2))\n",
        "    summary = analyzer.generate_summary_report()\n",
        "\n",
        "    summary_data = [\n",
        "        [_prepare_arabic_text('الإجمالي'), _prepare_arabic_text('القيمة')],\n",
        "        [_prepare_arabic_text('إجمالي عدد المنافسات'), f\"{summary.get('Total Tenders', 'N/A'):,}\" if isinstance(summary.get('Total Tenders'), (int, float)) else str(summary.get('Total Tenders', 'N/A'))],\n",
        "        [_prepare_arabic_text('عدد الأنشطة المختلفة'), f\"{summary.get('Unique Activities', 'N/A'):,}\" if isinstance(summary.get('Unique Activities'), (int, float)) else str(summary.get('Unique Activities', 'N/A'))],\n",
        "        [_prepare_arabic_text('النشاط الأكثر شيوعاً'), summary.get('Most Common Activity', 'N/A')],\n",
        "        [_prepare_arabic_text('عدد الجهات'), f\"{summary.get('Unique Organizations', 'N/A'):,}\" if isinstance(summary.get('Unique Organizations'), (int, float)) else str(summary.get('Unique Organizations', 'N/A'))],\n",
        "        [_prepare_arabic_text('إجمالي قيمة الكراسه'), f\"{summary.get('Total Value', 'N/A'):,.0f}\" if isinstance(summary.get('Total Value'), (int, float)) else str(summary.get('Total Value', 'N/A'))],\n",
        "        [_prepare_arabic_text('متوسط قيمة الكراسه'), f\"{summary.get('Average Value', 'N/A'):,.0f}\" if isinstance(summary.get('Average Value'), (int, float)) else str(summary.get('Average Value', 'N/A'))],\n",
        "        [_prepare_arabic_text('أعلى قيمة كراسه'), f\"{summary.get('Max Value', 'N/A'):,.0f}\" if isinstance(summary.get('Max Value'), (int, float)) else str(summary.get('Max Value', 'N/A'))],\n",
        "        [_prepare_arabic_text('أقل قيمة كراسه'), f\"{summary.get('Min Value', 'N/A'):,.0f}\" if isinstance(summary.get('Min Value'), (int, float)) else str(summary.get('Min Value', 'N/A'))],\n",
        "        [_prepare_arabic_text('الفترة الزمنية (يوم)'), f\"{summary.get('Time Period Days', 'N/A'):,}\" if isinstance(summary.get('Time Period Days'), (int, float)) else str(summary.get('Time Period Days', 'N/A'))]\n",
        "    ]\n",
        "    summary_table = Table(summary_data, colWidths=[2.5 * inch, 2.5 * inch])\n",
        "    summary_table.setStyle(table_style)\n",
        "    story.append(Paragraph(_prepare_arabic_text('ملخص البيانات:'), arabic_style_normal))\n",
        "    story.append(summary_table)\n",
        "    story.append(Spacer(1, 0.25 * inch))\n",
        "\n",
        "    story.append(Paragraph(_prepare_arabic_text('2. التحليل حسب النشاط الأساسي'), arabic_style_heading2))\n",
        "    _, activity_stats = analyzer.group_by_activity()\n",
        "\n",
        "    if activity_stats is not None and not activity_stats.empty:\n",
        "        story.append(Paragraph(_prepare_arabic_text('إحصائيات مفصلة حسب النشاط:'), arabic_style_normal))\n",
        "        activity_table_data = df_to_table_data(activity_stats, include_index=True)\n",
        "        num_activity_cols = len(activity_stats.columns) + 1\n",
        "        activity_table = Table(activity_table_data, colWidths=[2 * inch] + [5.5 * inch / (num_activity_cols - 1)] * (num_activity_cols - 1))\n",
        "        activity_table.setStyle(table_style)\n",
        "        story.append(activity_table)\n",
        "        story.append(Spacer(1, 0.25 * inch))\n",
        "    elif activity_stats is None:\n",
        "        story.append(Paragraph(_prepare_arabic_text('بيانات النشاط الأساسي غير متوفرة.'), arabic_style_normal))\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('لا توجد إحصائيات نشاط لعرضها.'), arabic_style_normal))\n",
        "\n",
        "    if 'activity_plot' in analyzer.plot_paths and analyzer.plot_paths['activity_plot'].getbuffer().nbytes > 0:\n",
        "        story.append(Paragraph(_prepare_arabic_text('مقارنة عدد المناقصات وقيمة الكراسات حسب النشاط:'), arabic_style_normal))\n",
        "        img = RlImage(analyzer.plot_paths['activity_plot'], width=6*inch, height=3*inch)\n",
        "        story.append(img)\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('الرسم البياني للنشاط غير متوفر.'), arabic_style_normal))\n",
        "\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    story.append(Paragraph(_prepare_arabic_text('3. التحليل حسب الجهة المصدرة'), arabic_style_heading2))\n",
        "    org_counts = analyzer.group_by_organization()\n",
        "\n",
        "    if org_counts is not None and not org_counts.empty:\n",
        "        top_10_orgs = org_counts.head(10).to_frame(name='Count')\n",
        "        story.append(Paragraph(_prepare_arabic_text('أكثر 10 جهات إصداراً للمناقصات:'), arabic_style_normal))\n",
        "        org_table_data = df_to_table_data(top_10_orgs, include_index=True)\n",
        "        org_table = Table(org_table_data, colWidths=[4 * inch, 1.5 * inch])\n",
        "        org_table.setStyle(table_style)\n",
        "        story.append(org_table)\n",
        "        story.append(Spacer(1, 0.25 * inch))\n",
        "    elif org_counts is None:\n",
        "        story.append(Paragraph(_prepare_arabic_text('بيانات الجهة المصدرة غير متوفرة.'), arabic_style_normal))\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('لا توجد بيانات جهات لعرضها.'), arabic_style_normal))\n",
        "\n",
        "    if 'organization_plot' in analyzer.plot_paths and analyzer.plot_paths['organization_plot'].getbuffer().nbytes > 0:\n",
        "        story.append(Paragraph(_prepare_arabic_text('توزيع المناقصات حسب الجهة (أعلى 10):'), arabic_style_normal))\n",
        "        img = RlImage(analyzer.plot_paths['organization_plot'], width=6*inch, height=4*inch)\n",
        "        story.append(img)\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('الرسم البياني للجهات غير متوفر.'), arabic_style_normal))\n",
        "\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    story.append(Paragraph(_prepare_arabic_text('4. التحليل الزمني'), arabic_style_heading2))\n",
        "    analyzer.analyze_by_date()\n",
        "\n",
        "    if 'date_analysis_plot' in analyzer.plot_paths and analyzer.plot_paths['date_analysis_plot'].getbuffer().nbytes > 0:\n",
        "        story.append(Paragraph(_prepare_arabic_text('اتجاهات النشر الشهري وتوزيعها حسب أيام الأسبوع:'), arabic_style_normal))\n",
        "        img = RlImage(analyzer.plot_paths['date_analysis_plot'], width=6.5*inch, height=3*inch)\n",
        "        story.append(img)\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('الرسم البياني للتحليل الزمني غير متوفر.'), arabic_style_normal))\n",
        "\n",
        "    story.append(Spacer(1, 0.5 * inch))\n",
        "\n",
        "    analyzer.analyze_weekly_tender_count_trend()\n",
        "    if 'weekly_count_trend_plot' in analyzer.plot_paths and analyzer.plot_paths['weekly_count_trend_plot'].getbuffer().nbytes > 0:\n",
        "        story.append(Paragraph(_prepare_arabic_text('الاتجاه الأسبوعي لعدد المنافسات:'), arabic_style_normal))\n",
        "        img = RlImage(analyzer.plot_paths['weekly_count_trend_plot'], width=6.5*inch, height=3.5*inch)\n",
        "        story.append(img)\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('الرسم البياني لاتجاه العدد الأسبوعي غير متوفر.'), arabic_style_normal))\n",
        "\n",
        "    story.append(PageBreak())\n",
        "\n",
        "    story.append(Paragraph(_prepare_arabic_text('5. تحليل قيم الكراسه'), arabic_style_heading2))\n",
        "    value_stats, value_dist = analyzer.analyze_document_values()\n",
        "\n",
        "    if value_stats and any(isinstance(v, (int, float)) for v in value_stats.values()):\n",
        "        value_data_rows = [\n",
        "            [_prepare_arabic_text('إجمالي قيمة الكراسه'), f\"{value_stats.get('Total Value', 'N/A'):,.0f}\" if isinstance(value_stats.get('Total Value'), (int, float)) else str(value_stats.get('Total Value', 'N/A'))],\n",
        "            [_prepare_arabic_text('متوسط قيمة الكراسه'), f\"{value_stats.get('Average Value', 'N/A'):,.0f}\" if isinstance(value_stats.get('Average Value'), (int, float)) else str(value_stats.get('Average Value', 'N/A'))],\n",
        "            [_prepare_arabic_text('أعلى قيمة'), f\"{value_stats.get('Max Value', 'N/A'):,.0f}\" if isinstance(value_stats.get('Max Value'), (int, float)) else str(value_stats.get('Max Value', 'N/A'))],\n",
        "            [_prepare_arabic_text('أقل قيمة'), f\"{value_stats.get('Min Value', 'N/A'):,.0f}\" if isinstance(value_stats.get('Min Value'), (int, float)) else str(value_stats.get('Min Value', 'N/A'))]\n",
        "        ]\n",
        "        value_table = Table(value_data_rows, colWidths=[2.5 * inch, 2.5 * inch])\n",
        "        value_table.setStyle(table_style)\n",
        "        story.append(Paragraph(_prepare_arabic_text('ملخص إحصائيات قيم الكراسه:'), arabic_style_normal))\n",
        "        story.append(value_table)\n",
        "        story.append(Spacer(1, 0.25 * inch))\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('إحصائيات قيم الكراسه غير متوفرة.'), arabic_style_normal))\n",
        "\n",
        "    if 'value_analysis_plot' in analyzer.plot_paths and analyzer.plot_paths['value_analysis_plot'].getbuffer().nbytes > 0:\n",
        "        story.append(Paragraph(_prepare_arabic_text('توزيع القيم الإجمالية حسب الفئات:'), arabic_style_normal))\n",
        "        img = RlImage(analyzer.plot_paths['value_analysis_plot'], width=6.5*inch, height=3*inch)\n",
        "        story.append(img)\n",
        "    else:\n",
        "        story.append(Paragraph(_prepare_arabic_text('الرسم البياني لتوزيع قيم الكراسه غير متوفر.'), arabic_style_normal))\n",
        "\n",
        "    story.append(Spacer(1, 0.5 * inch))\n",
        "    story.append(Paragraph(_prepare_arabic_text('Hued & Araamis report'), arabic_style_normal))\n",
        "\n",
        "    doc.build(story)\n",
        "    print(f\"Report generated: {filename}\")\n",
        "    return filename\n",
        "\n",
        "# --- 6. EMAIL SENDING FUNCTION ---\n",
        "def send_email_with_pdf(pdf_filepath, recipient_email):\n",
        "    if not SENDER_EMAIL or not SENDER_PASSWORD:\n",
        "        print(\"Email settings are missing.\")\n",
        "        return\n",
        "\n",
        "    message = MIMEMultipart()\n",
        "    message[\"From\"] = SENDER_EMAIL\n",
        "    message[\"To\"] = recipient_email\n",
        "    message[\"Subject\"] = \"تقرير تحليل المنافسات\"\n",
        "\n",
        "    body = \"مرفق تقرير تحليل المنافسات بصيغة PDF.\"\n",
        "    message.attach(MIMEText(body, \"plain\", \"utf-8\"))\n",
        "\n",
        "    try:\n",
        "        with open(pdf_filepath, \"rb\") as attachment:\n",
        "            part = MIMEBase(\"application\", \"octet-stream\")\n",
        "            part.set_payload(attachment.read())\n",
        "        encoders.encode_base64(part)\n",
        "        part.add_header(\"Content-Disposition\", f\"attachment; filename={pdf_filepath}\")\n",
        "        message.attach(part)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Attachment not found: {pdf_filepath}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error attaching file: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        context = ssl.create_default_context()\n",
        "        with smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT, context=context) as server:\n",
        "            server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
        "            server.sendmail(SENDER_EMAIL, recipient_email, message.as_string())\n",
        "        print(f\"Email sent to {recipient_email}\")\n",
        "    except Exception as e:\n",
        "        print(f\"SMTP error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8N8Vonih3EyH",
      "metadata": {
        "id": "8N8Vonih3EyH"
      },
      "outputs": [],
      "source": [
        "# Build analyzer and load data (expects a DataFrame-compatible object)\n",
        "analyzer = TenderDataAnalyzer()\n",
        "analyzer.load_data_from_text(data)\n",
        "\n",
        "# Generate PDF report\n",
        "REPORT_FILE = f\"Tender_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
        "generated_pdf_path = create_full_pdf_report(analyzer, REPORT_FILE)\n",
        "\n",
        "# Send report by email\n",
        "RECIPIENT_EMAIL = \"basma66yy@gmail.com\"\n",
        "send_email_with_pdf(generated_pdf_path, RECIPIENT_EMAIL)\n",
        "\n",
        "print(generated_pdf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wTZ0TbyI8JsT",
      "metadata": {
        "id": "wTZ0TbyI8JsT"
      },
      "source": [
        "##Machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QK67nuXx9J_1",
      "metadata": {
        "id": "QK67nuXx9J_1"
      },
      "outputs": [],
      "source": [
        "for i, col_name in enumerate(list(data.columns)):\n",
        "    print(f\"Index: {i}, Column Name: {col_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P3K6mu-kivuP",
      "metadata": {
        "id": "P3K6mu-kivuP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Tuple\n",
        "\n",
        "def preprocess_data(df_main: pd.DataFrame, df_target: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Merge, clean, and engineer features for tender data.\n",
        "\n",
        "    Requirements/assumptions:\n",
        "      - df_main contains tender fields including:\n",
        "        ['الرقم المرجعي','النشاط الاساسي','العنوان','وصف',\n",
        "         'قيمه وثائق المنافسه','اخر موعد لتقديم العرض','تاريخ النشر']\n",
        "        (some may be missing; code guards accordingly)\n",
        "      - df_target contains ['الرقم المرجعي','حالة القبول'] (target label)\n",
        "\n",
        "    Returns:\n",
        "      X (features DataFrame), y (target Series)\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1) Merge and basic cleanup\")\n",
        "\n",
        "    # Defensive copies\n",
        "    df_main = df_main.copy()\n",
        "    df_target = df_target.copy()\n",
        "\n",
        "    # Ensure required merge keys exist\n",
        "    if 'الرقم المرجعي' not in df_main.columns or 'الرقم المرجعي' not in df_target.columns:\n",
        "        raise KeyError(\"Both dataframes must include 'الرقم المرجعي' for merging.\")\n",
        "\n",
        "    if 'حالة القبول' not in df_target.columns:\n",
        "        raise KeyError(\"df_target must include 'حالة القبول' as the classification label.\")\n",
        "\n",
        "    # Inner join\n",
        "    df = pd.merge(df_main, df_target[['الرقم المرجعي', 'حالة القبول']], on='الرقم المرجعي', how='inner')\n",
        "    print(f\"Rows after inner join: {len(df)}\")\n",
        "\n",
        "    # Normalize common alternative column names\n",
        "    # Map alternatives to canonical names used below\n",
        "    rename_map = {\n",
        "        'اخر معاد لاستلام الاستفسارات': 'اخر موعد لاستلام الاستفسارات',\n",
        "        'قيمة وثائق المنافسة': 'قيمه وثائق المنافسه',\n",
        "        'قيمة وثائق المنافسه': 'قيمه وثائق المنافسه',\n",
        "        'URL': 'url',\n",
        "        'الرابط': 'url'\n",
        "    }\n",
        "    present_map = {k: v for k, v in rename_map.items() if k in df.columns}\n",
        "    if present_map:\n",
        "        df = df.rename(columns=present_map)\n",
        "\n",
        "    # Drop unneeded columns if they exist\n",
        "    to_drop = [c for c in ['url', 'اخر موعد لاستلام الاستفسارات'] if c in df.columns]\n",
        "    if to_drop:\n",
        "        df = df.drop(columns=to_drop)\n",
        "        print(f\"Dropped columns: {to_drop}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    print(\"2) Value cleaning and date features\")\n",
        "\n",
        "    # Clean numeric field: 'قيمه وثائق المنافسه'\n",
        "    val_col = 'قيمه وثائق المنافسه'\n",
        "    if val_col in df.columns:\n",
        "        # Replace Arabic \"free\" variants with 0\n",
        "        ser = (\n",
        "            df[val_col]\n",
        "            .astype(str)\n",
        "            .str.replace(r'مجانا|مجان', '0', flags=re.IGNORECASE, regex=True)\n",
        "            # Strip any non-digit separators except dot and minus\n",
        "            .str.replace(r'[^\\d\\.\\-]', '', regex=True)\n",
        "            .str.strip()\n",
        "        )\n",
        "        df[val_col] = pd.to_numeric(ser, errors='coerce').fillna(0)\n",
        "    else:\n",
        "        df[val_col] = 0.0\n",
        "\n",
        "    # Parse dates (coerce invalids)\n",
        "    date_cols = ['اخر موعد لتقديم العرض', 'تاريخ النشر']\n",
        "    for c in date_cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce')\n",
        "        else:\n",
        "            df[c] = pd.NaT\n",
        "\n",
        "    # Feature: days remaining from publish to deadline (can be negative if data inconsistent)\n",
        "    df['ايام_متبقية_للعرض'] = (df['اخر موعد لتقديم العرض'] - df['تاريخ النشر']).dt.days\n",
        "    df['ايام_متبقية_للعرض'] = df['ايام_متبقية_للعرض'].fillna(0).astype(int)\n",
        "\n",
        "    # Feature: free flag\n",
        "    df['مجانية_المنافسة'] = (df[val_col] == 0).astype(int)\n",
        "\n",
        "    # Remove raw dates if not needed further\n",
        "    df = df.drop(columns=[c for c in date_cols if c in df.columns], errors='ignore')\n",
        "    print(\"Engineered: 'ايام├متبقية├للعرض', 'مجانية├المنافسة' and cleaned value column.\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    print(\"3) Categorical encoding\")\n",
        "\n",
        "    # Ensure categorical columns exist; create if absent to keep pipeline stable\n",
        "    for c in ['النشاط الاساسي', 'العنوان', 'وصف']:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "\n",
        "    # Fill NaNs then one-hot encode (drop_first to limit dummy explosion)\n",
        "    cat_cols = ['النشاط الاساسي', 'العنوان', 'وصف']\n",
        "    df[cat_cols] = df[cat_cols].fillna(\"\").astype(str)\n",
        "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "    # Target encoding\n",
        "    # Map to binary (customize if different labels are used)\n",
        "    label_map = {'مقبول': 1, 'مرفوض': 0}\n",
        "    y = df['حالة القبول'].map(label_map)\n",
        "    if y.isna().any():\n",
        "        # Fallback: treat any non-مرفوض as 1, only explicit مرفوض as 0\n",
        "        y = (df['حالة القبول'] != 'مرفوض').astype(int)\n",
        "    df = df.drop(columns=['حالة القبول'])\n",
        "\n",
        "    # Remove identifier columns not intended as features\n",
        "    if 'الرقم المرجعي' in df.columns:\n",
        "        df = df.drop(columns=['الرقم المرجعي'])\n",
        "\n",
        "    X = df\n",
        "\n",
        "    print(f\"Final shape -> X: {X.shape}, y: {y.shape}\")\n",
        "    print(f\"Feature sample: {list(X.columns)[:8]} ...\")\n",
        "    print(\"=\" * 60)\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rcz6tJfM_TOu",
      "metadata": {
        "id": "Rcz6tJfM_TOu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Arabic rendering helper (safe fallback if libraries غير متاحة)\n",
        "def _prepare_arabic_text(text):\n",
        "    try:\n",
        "        import arabic_reshaper\n",
        "        from bidi.algorithm import get_display\n",
        "        if not isinstance(text, str):\n",
        "            return text\n",
        "        if not text.strip():\n",
        "            return text\n",
        "        return get_display(arabic_reshaper.reshape(text))\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "def get_column_total_values(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    يجمع قيمة الأعمدة الرقمية. لو العمود نصي لكنه أرقام (مثلاً 'مجانا' أو بها رموز)،\n",
        "    يحاول تحويله لأرقام ثم يجمعه.\n",
        "    \"\"\"\n",
        "    totals = {}\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        if pd.api.types.is_numeric_dtype(s):\n",
        "            totals[col] = float(pd.to_numeric(s, errors='coerce').fillna(0).sum())\n",
        "        else:\n",
        "            # محاولة تحويل النص لأرقام (إزالة غير الأرقام و'مجانا' -> 0)\n",
        "            try:\n",
        "                ser = (\n",
        "                    s.astype(str)\n",
        "                     .str.replace(r'مجانا|مجان', '0', regex=True)\n",
        "                     .str.replace(r'[^\\d\\.\\-]', '', regex=True)\n",
        "                     .str.strip()\n",
        "                )\n",
        "                nums = pd.to_numeric(ser, errors='coerce').fillna(0)\n",
        "                if nums.notna().any() and nums.sum() != 0:\n",
        "                    totals[col] = float(nums.sum())\n",
        "            except Exception:\n",
        "                pass\n",
        "    return totals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pgrqhR_i_adb",
      "metadata": {
        "id": "pgrqhR_i_adb"
      },
      "outputs": [],
      "source": [
        "# Compute totals and pretty-print (sorted descending)\n",
        "column_name_and_totals = get_column_total_values(data)\n",
        "\n",
        "for col_name, total in sorted(column_name_and_totals.items(), key=lambda kv: kv[1], reverse=True):\n",
        "    name = _prepare_arabic_text(str(col_name))\n",
        "    try:\n",
        "        val = float(total)\n",
        "        if val.is_integer():\n",
        "            print(f\"Column Name: {name}, Total Value: {int(val)}\")\n",
        "        else:\n",
        "            print(f\"Column Name: {name}, Total Value: {val:.2f}\")\n",
        "    except Exception:\n",
        "        print(f\"Column Name: {name}, Total Value: {total}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624ac879",
      "metadata": {
        "id": "624ac879"
      },
      "outputs": [],
      "source": [
        "# Build a totals table from the previously computed dictionary\n",
        "totals = get_column_total_values(data)\n",
        "\n",
        "column_totals_df = (\n",
        "    pd.Series(totals, name=\"Total Value\")\n",
        "      .rename_axis(\"Column Name\")\n",
        "      .reset_index()\n",
        "      .sort_values(\"Total Value\", ascending=False, ignore_index=True)\n",
        ")\n",
        "\n",
        "display(column_totals_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t_gPl1wXjrtS",
      "metadata": {
        "id": "t_gPl1wXjrtS"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Dict, Any\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "def train_and_evaluate_model(\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    test_size: float = 0.3,\n",
        "    random_state: int = 42,\n",
        "    n_estimators: int = 300,\n",
        "    max_depth: int | None = None\n",
        ") -> Tuple[RandomForestClassifier, float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train a RandomForest classifier and evaluate it.\n",
        "    Handles edge cases such as single-class targets and small datasets.\n",
        "    \"\"\"\n",
        "    # Basic validations\n",
        "    if len(X) != len(y):\n",
        "        raise ValueError(\"X and y must have the same number of rows.\")\n",
        "    if len(X) < 3:\n",
        "        raise ValueError(\"Dataset too small to split.\")\n",
        "    y = y.astype(int)\n",
        "\n",
        "    # Stratify if both classes exist\n",
        "    classes = np.unique(y)\n",
        "    stratify_arg = y if len(classes) > 1 else None\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=stratify_arg\n",
        "    )\n",
        "\n",
        "    # If single-class in train, force a trivial classifier\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        majority = int(np.bincount(y).argmax())\n",
        "        class TrivialRF:\n",
        "            def predict(self, Z): return np.full(len(Z), majority, dtype=int)\n",
        "            feature_importances_ = np.zeros(X.shape[1], dtype=float)\n",
        "        model = TrivialRF()\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = float((y_pred == y_test).mean())\n",
        "        report = classification_report(y_test, y_pred, target_names=['0', '1'], zero_division=0, output_dict=True)\n",
        "        metrics = {\n",
        "            \"accuracy\": acc,\n",
        "            \"report\": report,\n",
        "            \"confusion_matrix\": confusion_matrix(y_test, y_pred).tolist(),\n",
        "            \"feature_importances_top5\": []\n",
        "        }\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_test, y_pred, target_names=['0','1'], zero_division=0))\n",
        "        return model, acc, metrics\n",
        "\n",
        "    # Train RF\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        random_state=random_state,\n",
        "        class_weight=\"balanced_subsample\",\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report_txt = classification_report(y_test, y_pred, target_names=['مرفوض (0)', 'مقبول (1)'], zero_division=0)\n",
        "    report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "    prfs = precision_recall_fscore_support(y_test, y_pred, zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Top 5 important features\n",
        "    importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "    top5 = importances.nlargest(5)\n",
        "\n",
        "    print(f\"Training size: {len(X_train)} | Test size: {len(X_test)}\")\n",
        "    print(\"Model: RandomForestClassifier\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\", report_txt)\n",
        "    print(\"\\nTop 5 Feature Importances:\")\n",
        "    print(top5.to_string())\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"report\": report,\n",
        "        \"precision_recall_fscore_support\": {\n",
        "            \"precision\": prfs[0].tolist(),\n",
        "            \"recall\": prfs[1].tolist(),\n",
        "            \"f1\": prfs[2].tolist(),\n",
        "            \"support\": prfs[3].tolist(),\n",
        "        },\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"feature_importances_top5\": top5.to_dict()\n",
        "    }\n",
        "    return model, acc, metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ASvR_H04kZZw",
      "metadata": {
        "id": "ASvR_H04kZZw"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "def predict_single_row(model, raw_row_data: Dict[str, Any], X_train_cols: list) -> Tuple[int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Preprocess a single raw row, align to training features, and predict.\n",
        "    Mirrors the steps in preprocess_data() to keep feature engineering consistent.\n",
        "    Returns (predicted_label, probabilities_array).\n",
        "    \"\"\"\n",
        "    # 1) Build DataFrame\n",
        "    df = pd.DataFrame([raw_row_data]).copy()\n",
        "\n",
        "    # 2) Normalize common alternative column names to canonical ones used in training\n",
        "    rename_map = {\n",
        "        'اخر معاد لاستلام الاستفسارات': 'اخر موعد لاستلام الاستفسارات',\n",
        "        'قيمة وثائق المنافسة': 'قيمه وثائق المنافسه',\n",
        "        'قيمة وثائق المنافسه': 'قيمه وثائق المنافسه',\n",
        "        'URL': 'url',\n",
        "        'الرابط': 'url'\n",
        "    }\n",
        "    present_map = {k: v for k, v in rename_map.items() if k in df.columns}\n",
        "    if present_map:\n",
        "        df = df.rename(columns=present_map)\n",
        "\n",
        "    # 3) Drop columns not used as features\n",
        "    drop_cols = ['url', 'اخر موعد لاستلام الاستفسارات', 'الرقم المرجعي', 'حالة القبول']\n",
        "    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    # 4) Clean numeric value column\n",
        "    val_col = 'قيمه وثائق المنافسه'\n",
        "    if val_col in df.columns:\n",
        "        ser = (\n",
        "            df[val_col].astype(str)\n",
        "            .str.replace(r'مجانا|مجان', '0', flags=re.IGNORECASE, regex=True)\n",
        "            .str.replace(r'[^\\d\\.\\-]', '', regex=True)\n",
        "            .str.strip()\n",
        "        )\n",
        "        df[val_col] = pd.to_numeric(ser, errors='coerce').fillna(0.0)\n",
        "    else:\n",
        "        df[val_col] = 0.0\n",
        "\n",
        "    # 5) Dates -> engineered features\n",
        "    date_cols = ['اخر موعد لتقديم العرض', 'تاريخ النشر']\n",
        "    for c in date_cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce')\n",
        "        else:\n",
        "            df[c] = pd.NaT\n",
        "\n",
        "    df['ايام_متبقية_للعرض'] = (df['اخر موعد لتقديم العرض'] - df['تاريخ النشر']).dt.days\n",
        "    df['ايام_متبقية_للعرض'] = df['ايام_متبقية_للعرض'].fillna(0).astype(int)\n",
        "\n",
        "    df['مجانية_المنافسة'] = (df[val_col] == 0).astype(int)\n",
        "\n",
        "    df = df.drop(columns=[c for c in date_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    # 6) Categorical one-hot encoding\n",
        "    cat_cols = ['النشاط الاساسي', 'العنوان', 'وصف']\n",
        "    for c in cat_cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    df[cat_cols] = df[cat_cols].fillna(\"\").astype(str)\n",
        "\n",
        "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "    # 7) Align to training feature columns\n",
        "    df_aligned = df.reindex(columns=X_train_cols, fill_value=0)\n",
        "\n",
        "    # 8) Predict\n",
        "    pred = int(model.predict(df_aligned)[0])\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba = model.predict_proba(df_aligned)[0]\n",
        "    else:\n",
        "        # Fallback: pseudo-proba for models without predict_proba\n",
        "        proba = np.array([1.0 - float(pred), float(pred)], dtype=float)\n",
        "\n",
        "    return pred, proba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y06Nbmuat_lH",
      "metadata": {
        "id": "Y06Nbmuat_lH"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "ws = gc.open_by_key(SHEET_ID).sheet1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Read target sheet (OAuth path, no credentials.json) ---\n",
        "# assumes: gc is already defined from auth.authenticate_user()\n",
        "import pandas as pd\n",
        "\n",
        "TARGET_SHEET_ID = \"15F9xOSLfXDuIQBDIhbW2g6ZFWlkTRmEX2507KCSP5UI\"\n",
        "TARGET_TAB = \"Sheet1\"  # change if different\n",
        "\n",
        "wb2 = gc.open_by_key(TARGET_SHEET_ID)\n",
        "tabs = [w.title for w in wb2.worksheets()]\n",
        "ws2 = wb2.worksheet(TARGET_TAB) if TARGET_TAB in tabs else wb2.sheet1\n",
        "\n",
        "rows = ws2.get_all_records()         # row 1 = headers\n",
        "df_target = pd.DataFrame(rows)\n",
        "\n",
        "# normalize headers to canonical names used downstream\n",
        "rename_map = {\n",
        "    \"URL\": \"url\",\n",
        "    \"الرابط\": \"url\",\n",
        "    \"اخر معاد لاستلام الاستفسارات\": \"اخر موعد لاستلام الاستفسارات\",\n",
        "    \"قيمة وثائق المنافسة\": \"قيمه وثائق المنافسه\",\n",
        "    \"قيمة وثائق المنافسه\": \"قيمه وثائق المنافسه\",\n",
        "}\n",
        "present = {k: v for k, v in rename_map.items() if k in df_target.columns}\n",
        "if present:\n",
        "    df_target = df_target.rename(columns=present)\n",
        "\n",
        "# keep only needed columns if present\n",
        "needed = [\"الرقم المرجعي\", \"حالة القبول\"]\n",
        "missing = [c for c in needed if c not in df_target.columns]\n",
        "if missing:\n",
        "    print(f\"Warning: missing columns in target sheet: {missing}\")\n",
        "else:\n",
        "    for c in needed:\n",
        "        df_target[c] = df_target[c].astype(str).str.strip()\n",
        "\n",
        "print(f\"Loaded target sheet rows: {len(df_target)} | cols: {len(df_target.columns)}\")\n",
        "display(df_target.head())\n"
      ],
      "metadata": {
        "id": "t21DTRqEKvm3"
      },
      "id": "t21DTRqEKvm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define, normalize, and sanity-check join key in one shot ===\n",
        "import pandas as pd\n",
        "\n",
        "# 1) define df_main safely\n",
        "if 'data' in globals() and isinstance(data, pd.DataFrame):\n",
        "    df_main = data.copy()\n",
        "elif 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "    df_main = df.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"df_main source not found: expected a DataFrame named 'data' or 'df'.\")\n",
        "\n",
        "# 2) ensure df_target exists\n",
        "if 'df_target' not in globals() or not isinstance(df_target, pd.DataFrame):\n",
        "    raise RuntimeError(\"df_target is missing. Load your target sheet into a DataFrame named 'df_target' first.\")\n",
        "\n",
        "# 3) normalize the join key\n",
        "KEY_COL = 'الرقم المرجعي'\n",
        "\n",
        "def _normalize_key_col(df: pd.DataFrame, key=KEY_COL) -> pd.DataFrame:\n",
        "    if key not in df.columns:\n",
        "        raise KeyError(f\"Missing join key column: {key}\")\n",
        "    s = df[key].astype(str)\n",
        "    s = (s.str.replace('\\u200f','', regex=False)   # remove RTL marks\n",
        "           .str.replace(r'\\s+','', regex=True)     # remove spaces\n",
        "           .str.replace('٬','', regex=False)       # Arabic thousands sep\n",
        "           .str.replace('٫','.', regex=False)      # Arabic decimal\n",
        "           .str.replace(r'[^\\d]','', regex=True)   # keep digits only\n",
        "           .str.strip())\n",
        "    df[key] = s\n",
        "    return df\n",
        "\n",
        "df_main   = _normalize_key_col(df_main)\n",
        "df_target = _normalize_key_col(df_target)\n",
        "\n",
        "# 4) quick diagnostics (optional)\n",
        "km = set(df_main[KEY_COL].dropna())\n",
        "kt = set(df_target[KEY_COL].dropna())\n",
        "print(\"df_main rows:\", len(df_main), \"| unique keys:\", len(km))\n",
        "print(\"df_target rows:\", len(df_target), \"| unique keys:\", len(kt))\n",
        "print(\"intersection size:\", len(km & kt))\n"
      ],
      "metadata": {
        "id": "fTcjHcgz5TF9"
      },
      "id": "fTcjHcgz5TF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize join key in both dataframes to string\n",
        "def _normalize_key_col(df, key='الرقم المرجعي'):\n",
        "    if key not in df.columns:\n",
        "        raise KeyError(f\"Missing join key column: {key}\")\n",
        "    s = df[key].astype(str)\n",
        "    s = (s.str.replace('\\u200f', '', regex=False)       # remove RTL marks if any\n",
        "           .str.replace(r'\\s+', '', regex=True)         # remove spaces\n",
        "           .str.replace('٬', '', regex=False)           # thousands sep\n",
        "           .str.replace('٫', '.', regex=False)          # decimal sep\n",
        "           .str.replace(r'[^\\d]', '', regex=True)       # keep digits only (avoids float precision loss)\n",
        "           .str.strip())\n",
        "    df[key] = s\n",
        "    return df\n",
        "\n",
        "df_main = _normalize_key_col(df_main.copy(), 'الرقم المرجعي')\n",
        "df_target = _normalize_key_col(df_target.copy(), 'الرقم المرجعي')\n"
      ],
      "metadata": {
        "id": "8nRShf0OMHBI"
      },
      "id": "8nRShf0OMHBI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Safe join diagnose + fallback + optional training ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 0) Preconditions\n",
        "if 'data' in globals() and isinstance(data, pd.DataFrame):\n",
        "    df_main = data.copy()\n",
        "elif 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "    df_main = df.copy()\n",
        "else:\n",
        "    raise RuntimeError(\"Main DataFrame not found. Expected 'data' or 'df'.\")\n",
        "\n",
        "assert 'df_target' in globals() and isinstance(df_target, pd.DataFrame), \"df_target is missing.\"\n",
        "\n",
        "KEY_COL = 'الرقم المرجعي'\n",
        "\n",
        "def _norm(s: pd.Series) -> pd.Series:\n",
        "    return (s.astype(str)\n",
        "             .str.replace('\\u200f','', regex=False)\n",
        "             .str.replace(r'\\s+','', regex=True)\n",
        "             .str.replace('٬','', regex=False)\n",
        "             .str.replace('٫','.', regex=False)\n",
        "             .str.replace(r'[^\\d]','', regex=True)\n",
        "             .str.strip())\n",
        "\n",
        "# 1) Normalize keys\n",
        "if KEY_COL not in df_main.columns: raise KeyError(f\"Missing '{KEY_COL}' in df_main\")\n",
        "if KEY_COL not in df_target.columns: raise KeyError(f\"Missing '{KEY_COL}' in df_target\")\n",
        "dfm = df_main.copy(); dfm[KEY_COL] = _norm(dfm[KEY_COL])\n",
        "dft = df_target.copy(); dft[KEY_COL] = _norm(dft[KEY_COL])\n",
        "\n",
        "# 2) Direct intersection\n",
        "km = set(dfm[KEY_COL].dropna())\n",
        "kt = set(dft[KEY_COL].dropna())\n",
        "inter = km & kt\n",
        "print(f\"[Direct] main unique: {len(km)} | target unique: {len(kt)} | intersection: {len(inter)}\")\n",
        "\n",
        "# 3) If no intersection, try last-9-digits join (fallback)\n",
        "use_tail_join = False\n",
        "if len(inter) == 0:\n",
        "    dfm['_k9'] = dfm[KEY_COL].str[-9:]\n",
        "    dft['_k9'] = dft[KEY_COL].str[-9:]\n",
        "    inter9 = set(dfm['_k9']) & set(dft['_k9'])\n",
        "    print(f\"[Tail-9] intersection: {len(inter9)}\")\n",
        "    if len(inter9) > 0:\n",
        "        use_tail_join = True\n",
        "\n",
        "# 4) Build aligned labeled set\n",
        "if use_tail_join:\n",
        "    df_merged = pd.merge(\n",
        "        dfm, dft[['_k9','حالة القبول']], on='_k9', how='inner'\n",
        "    )\n",
        "    # reattach canonical key if needed\n",
        "    if KEY_COL not in df_merged.columns and KEY_COL in dfm.columns:\n",
        "        pass\n",
        "else:\n",
        "    df_merged = pd.merge(\n",
        "        dfm, dft[[KEY_COL,'حالة القبول']], on=KEY_COL, how='inner'\n",
        "    )\n",
        "\n",
        "print(f\"Merged labeled rows: {len(df_merged)}\")\n",
        "\n",
        "# 5) If too small, skip training gracefully\n",
        "if len(df_merged) < 3:\n",
        "    print(\"Not enough labeled rows to train a model. Skipping ML training.\")\n",
        "    trained_model, accuracy, metrics = None, None, {}\n",
        "else:\n",
        "    # Split back into df_main-like and df_target-like for your preprocess\n",
        "    cols_target = ['حالة القبول']\n",
        "    df_main_for_ml = df_merged.drop(columns=[c for c in cols_target if c in df_merged.columns]).copy()\n",
        "    df_target_for_ml = df_merged[[KEY_COL, 'حالة القبول']].copy() if KEY_COL in df_merged.columns else df_merged[['حالة القبول']].copy()\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if KEY_COL not in df_main_for_ml.columns and KEY_COL in dfm.columns:\n",
        "        df_main_for_ml[KEY_COL] = dfm[KEY_COL]\n",
        "\n",
        "    # Call your existing pipeline\n",
        "    X_features, y_target = preprocess_data(df_main_for_ml, df_target_for_ml)\n",
        "    if len(X_features) < 3:\n",
        "        print(\"Not enough rows after preprocessing. Skipping ML training.\")\n",
        "        trained_model, accuracy, metrics = None, None, {}\n",
        "    else:\n",
        "        trained_model, accuracy, metrics = train_and_evaluate_model(X_features, y_target)\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        if isinstance(metrics, dict) and 'report' in metrics:\n",
        "            print(\"Classes in report:\", list(metrics['report'].keys()))\n"
      ],
      "metadata": {
        "id": "9TpQTJMs-pDk"
      },
      "id": "9TpQTJMs-pDk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot: normalize key, preprocess, guard small data, train if possible\n",
        "import pandas as pd\n",
        "\n",
        "# 1) define main df safely\n",
        "df_main = data.copy() if 'data' in globals() else df.copy()\n",
        "\n",
        "# 2) ensure df_target exists\n",
        "assert 'df_target' in globals(), \"df_target is missing.\"\n",
        "\n",
        "KEY_COL = 'الرقم المرجعي'\n",
        "\n",
        "def _norm(s: pd.Series) -> pd.Series:\n",
        "    return (s.astype(str)\n",
        "             .str.replace('\\u200f','', regex=False)\n",
        "             .str.replace(r'\\s+','', regex=True)\n",
        "             .str.replace('٬','', regex=False)\n",
        "             .str.replace('٫','.', regex=False)\n",
        "             .str.replace(r'[^\\d]','', regex=True)\n",
        "             .str.strip())\n",
        "\n",
        "# 3) normalize join key types\n",
        "if KEY_COL not in df_main.columns: raise KeyError(f\"Missing '{KEY_COL}' in df_main\")\n",
        "if KEY_COL not in df_target.columns: raise KeyError(f\"Missing '{KEY_COL}' in df_target\")\n",
        "df_main[KEY_COL]   = _norm(df_main[KEY_COL])\n",
        "df_target[KEY_COL] = _norm(df_target[KEY_COL])\n",
        "\n",
        "# 4) quick intersection check to avoid empty merge\n",
        "inter = set(df_main[KEY_COL]) & set(df_target[KEY_COL])\n",
        "if len(inter) < 3:\n",
        "    print(f\"Not enough labeled intersection ({len(inter)}). Skipping ML.\")\n",
        "    trained_model, accuracy, metrics = None, None, {}\n",
        "    X_features, y_target, X_train_cols = pd.DataFrame(), pd.Series(dtype=int), []\n",
        "else:\n",
        "    X_features, y_target = preprocess_data(df_main, df_target)\n",
        "    if len(X_features) < 3:\n",
        "        print(\"Not enough rows after preprocessing. Skipping ML.\")\n",
        "        trained_model, accuracy, metrics = None, None, {}\n",
        "        X_train_cols = []\n",
        "    else:\n",
        "        trained_model, accuracy, metrics = train_and_evaluate_model(X_features, y_target)\n",
        "        X_train_cols = X_features.columns.tolist()\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        if isinstance(metrics, dict) and \"report\" in metrics:\n",
        "            print(\"Classes in report:\", list(metrics[\"report\"].keys()))\n"
      ],
      "metadata": {
        "id": "EoimHJIBB1VU"
      },
      "id": "EoimHJIBB1VU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Locate a valid row by reference id (interactive fallback) ---\n",
        "\n",
        "import pandas as pd\n",
        "from difflib import get_close_matches\n",
        "\n",
        "# 1) choose main df\n",
        "base_df = data if 'data' in globals() else df_main\n",
        "\n",
        "REF_COL = 'الرقم المرجعي'\n",
        "assert REF_COL in base_df.columns, f\"Missing column: {REF_COL}\"\n",
        "\n",
        "# 2) normalize helper\n",
        "def norm_digits(s):\n",
        "    s = str(s)\n",
        "    for a,b in [('‏',''), (' ','')]: s = s.replace(a,b)\n",
        "    s = s.replace('٬','').replace('٫','.')\n",
        "    s = ''.join(ch for ch in s if ch.isdigit())\n",
        "    return s.strip()\n",
        "\n",
        "dfN = base_df.copy()\n",
        "dfN['_ref_norm'] = dfN[REF_COL].astype(str).map(norm_digits)\n",
        "\n",
        "# 3) candidate list\n",
        "cands = dfN['_ref_norm'].dropna().unique().tolist()\n",
        "print(f\"Available refs ({len(cands)}):\", cands[:10], (\"...\" if len(cands)>10 else \"\"))\n",
        "\n",
        "# 4) put your desired id here; if empty, we'll pick the first\n",
        "TARGET_ID = \"\"  # e.g. \"251039012564\"\n",
        "TARGET_ID = norm_digits(TARGET_ID) if TARGET_ID else \"\"\n",
        "\n",
        "row = None\n",
        "reason = None\n",
        "\n",
        "if TARGET_ID and TARGET_ID in cands:\n",
        "    row = dfN[dfN['_ref_norm'] == TARGET_ID].iloc[0]\n",
        "    reason = \"exact\"\n",
        "elif TARGET_ID:\n",
        "    # try close matches (string similarity)\n",
        "    close = get_close_matches(TARGET_ID, cands, n=1, cutoff=0.8)\n",
        "    if close:\n",
        "        row = dfN[dfN['_ref_norm'] == close[0]].iloc[0]\n",
        "        reason = f\"closest:{close[0]}\"\n",
        "    else:\n",
        "        # fallback to first candidate if nothing close\n",
        "        row = dfN.iloc[0]\n",
        "        reason = \"fallback:first-row\"\n",
        "else:\n",
        "    # user didn't specify; just take first row\n",
        "    row = dfN.iloc[0]\n",
        "    reason = \"default:first-row\"\n",
        "\n",
        "print(f\"Selected row reason: {reason}\")\n",
        "print(\"Selected ref:\", row['_ref_norm'])\n",
        "\n",
        "raw_value = row.drop(labels=['_ref_norm']).to_dict()\n",
        "\n",
        "# 5) predict only if model trained; otherwise just show the row\n",
        "can_predict = (\n",
        "    'trained_model' in globals() and trained_model is not None and\n",
        "    'X_train_cols' in globals() and isinstance(X_train_cols, list) and len(X_train_cols) > 0\n",
        ")\n",
        "\n",
        "if can_predict:\n",
        "    pred_label, proba = predict_single_row(trained_model, raw_value, X_train_cols)\n",
        "    label_text = \"مقبول\" if pred_label == 1 else \"مرفوض\"\n",
        "    confidence = float(proba[pred_label])\n",
        "    print(\"\\n--- Prediction ---\")\n",
        "    print(f\"Label: {label_text} | Confidence: {confidence:.2%}\")\n",
        "else:\n",
        "    print(\"\\nModel not trained or X_train_cols missing. Showing selected row only:\")\n",
        "\n",
        "import pprint; pprint.pprint(raw_value)\n"
      ],
      "metadata": {
        "id": "gnyHl8MYEeax"
      },
      "id": "gnyHl8MYEeax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build analyzer and load data\n",
        "analyzer = TenderDataAnalyzer()\n",
        "analyzer.load_data_from_text(data)  # أو df_main لو ده اسمك\n",
        "\n",
        "# Generate PDF report\n",
        "REPORT_FILE = f\"Tender_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
        "generated_pdf_path = create_full_pdf_report(analyzer, REPORT_FILE)\n",
        "print(\"PDF ready at:\", generated_pdf_path)\n",
        "\n",
        "# Send report by email (ضبطي الإيميل/كلمة مرور التطبيق في الإعدادات)\n",
        "RECIPIENT_EMAIL = \"basma66yy@gmail.com\"\n",
        "send_email_with_pdf(generated_pdf_path, RECIPIENT_EMAIL)\n"
      ],
      "metadata": {
        "id": "oqWmH4a_F2Lc"
      },
      "id": "oqWmH4a_F2Lc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}